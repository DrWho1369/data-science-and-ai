<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lecture 3: Matrix Operations, Eigendecomposition & SVD</title>
    <link rel="stylesheet" href="afds-styles.css">
</head>
<body>
    <header>
        <h1>Lecture 3: Matrix Operations, Eigendecomposition & SVD</h1>
        <a href="afds_index.html" class="home-button">‚¨Ö Back to Module</a>
    </header>

    <!-- Key Learning Objectives -->
    <section id="key-objectives">
        <h2>Key Learning Objectives</h2>
        <ul>
            <li>Understand how to multiply matrices and the logic behind it.</li>
            <li>Learn about matrix transposition and its significance.</li>
            <li>Explore the inverse of a matrix and its applications.</li>
            <li>Understand vector decomposition and orthogonality.</li>
            <li>Learn the concepts of eigendecomposition and spectral decomposition.</li>
            <li>Define singular values and their relationship with eigenvalues.</li>
            <li>Understand Singular Value Decomposition (SVD) and its components.</li>
            <li>Apply matrix decompositions to real-world applications like data compression and PCA.</li>
        </ul>
    </section>

    <!-- Flashcards Section -->
    <section id="flashcards">
        <h2>Flashcards</h2>
        <div class="flashcard-container">
            <div class="flashcard" data-question="What is an eigenvector?" data-answer="A vector that remains in the same direction after a linear transformation, scaled by an eigenvalue."></div>
            <div class="flashcard" data-question="What is spectral decomposition?" data-answer="The decomposition of a symmetric matrix into eigenvalues and eigenvectors using an orthogonal matrix." ></div>
            <div class="flashcard" data-question="What is a singular value?" data-answer="A singular value is the square root of an eigenvalue of the Gram matrix (A·µÄA)."></div>
            <div class="flashcard" data-question="What is Singular Value Decomposition (SVD)?" data-answer="SVD decomposes a matrix A into three matrices: A = UŒ£V·µÄ." ></div>
            <div class="flashcard" data-question="Why is the Gram matrix useful in SVD?" data-answer="It allows us to compute singular values as the square roots of eigenvalues of A·µÄA." ></div>
            <div class="flashcard" data-question="What are left and right singular vectors?" data-answer="Left singular vectors are columns of U, and right singular vectors are columns of V in SVD." ></div>
            <div class="flashcard" data-question="Why is SVD important in PCA?" data-answer="SVD helps identify principal components of a dataset by decomposing the covariance matrix." ></div>
            <div class="flashcard" data-question="What is an orthogonal matrix?" data-answer="A square matrix whose columns and rows are orthonormal, meaning its transpose is also its inverse." ></div>
            <div class="flashcard" data-question="What does the diagonal matrix Œ£ contain in SVD?" data-answer="The singular values of the matrix A, which represent its scaling factors along orthogonal directions." ></div>
            <div class="flashcard" data-question="How can SVD be used in image compression?" data-answer="By keeping only the largest singular values and truncating the lower ones, reducing data size while maintaining structure." ></div>
            <div class="flashcard" data-question="What does it mean for a matrix to be diagonalizable?" data-answer="A matrix is diagonalizable if it can be written as PDP‚Åª¬π, where P contains eigenvectors and D contains eigenvalues." ></div>
            <div class="flashcard" data-question="Why does matrix multiplication not always commute?" data-answer="Because AB ‚â† BA in general, unless both matrices are diagonal or satisfy special properties." ></div>
            <div class="flashcard" data-question="How is the determinant of a matrix related to eigenvalues?" data-answer="The determinant of A is the product of its eigenvalues." ></div>
            <div class="flashcard" data-question="What happens when an eigenvalue is zero?" data-answer="The matrix is singular (non-invertible) and has a nontrivial null space." ></div>
        </div>
    </section>

    <!-- Revision Notes -->
    <section id="revision-notes">
        <h2>üü¢ Matrix Multiplication</h2>
        <p>Matrix multiplication is the process of multiplying two matrices, which results in a new matrix. If <code>A</code> is an <code>m √ó n</code> matrix and <code>B</code> is an <code>n √ó p</code> matrix, their product <code>C</code> is an <code>m √ó p</code> matrix.</p>
        <p>The entry at position <code>(i, j)</code> in the resulting matrix is computed as:</p>
        <pre><code>c_ij = a_i1 * b_1j + a_i2 * b_2j + ... + a_in * b_nj</code></pre>
        
        <h3>Example</h3>
        <pre><code>A = | 1  2  3 |     B = | 2  0 |
    | 4  5  6 |         | 1  3 |
                        | 0  4 |

C = A * B = | 4  18 |
            | 13 39 |</code></pre>
        
        <h2>üü¢ Vector Decomposition</h2>
        <p>Vector decomposition allows us to express a vector as a <strong>linear combination</strong> of orthogonal basis vectors.</p>
        <pre><code>u = a * v + b * w</code></pre>
        <p>where:</p>
        <ul>
            <li><code>v</code> and <code>w</code> are <strong>orthogonal unit vectors</strong>.</li>
            <li><code>a</code> and <code>b</code> are computed using <strong>dot products</strong>.</li>
        </ul>
        
        <h2>üü¢ Eigendecomposition</h2>
        <p>Eigendecomposition is a way of <strong>breaking down</strong> a matrix into <strong>its eigenvalues and eigenvectors</strong>.</p>
        <pre><code>A = P D P‚Åª¬π</code></pre>
        <p>Where:</p>
        <ul>
            <li>P is the matrix of eigenvectors - The <strong>columns of P</strong> are the <strong>eigenvectors</strong> of A.</li>
            <li>D is a diagonal matrix where the <strong>diagonal entries of D</strong> are the <strong>eigenvalues</strong> of A.</li>
        </ul>
        
        <h2>üü¢ Spectral Decomposition</h2>
        <p>For a <strong>symmetric matrix</strong>, (ùê¥ = ùê¥<sup>ùëá</sup>), eigendecomposition simplifies to:</p>
        <pre><code>A = P D P<sup>T</sup></code></pre>
        <h3>1. Why is This Important?</h3>
        <p></p>In spectral decomposition, ùëÉ is an orthogonal matrix, meaning: ùëÉ<sup>‚àí1</sup> = ùëÉ<sup>ùëá</sup> This makes computations simpler and more stable in numerical applications. Used in Principal Component Analysis (PCA) for reducing dimensions in machine learning.
        <h2>üü¢ Example of Spectral Decomposition</h2>
        <p>Given a symmetric matrix:</p>
        <pre><code>
        A = | 2  -1 |
            | -1  2 |
        </code></pre>
        
        <p>We find the eigenvalues and eigenvectors, then construct:</p>
        <pre><code>
        A = P D P<sup>ùëá</sup>
        </code></pre>
        
        <p>where:</p>
        <ul>
            <li><strong>P</strong> contains orthonormal eigenvectors.</li>
            <li><strong>D</strong> is a diagonal matrix of eigenvalues.</li>
        </ul>
    </section>

    <!-- Summary Table -->
<section id="eigendecomposition-summary">
    <h2>üìå Summary of Eigendecomposition vs Spectral Decomposition</h2>
    <table>
        <tr>
            <th>Eigendecomposition</th>
            <th>Spectral Decomposition</th>
        </tr>
        <tr>
            <td>Works for any square matrix A</td>
            <td>Works only for symmetric matrices A = A<sup>T</sup></td>
        </tr>
        <tr>
            <td>A = P D P<sup>-1</sup></td>
            <td>A = P D P<sup>T</sup></td>
        </tr>
        <tr>
            <td>P contains eigenvectors</td>
            <td>P contains orthonormal eigenvectors</td>
        </tr>
        <tr>
            <td>D contains eigenvalues</td>
            <td>D contains eigenvalues</td>
        </tr>
        <tr>
            <td>Used in solving systems, stability analysis</td>
            <td>Used in PCA, statistics, physics</td>
        </tr>
    </table>
</section>
<section>
        <h2>üü¢ Singular Value Decomposition (SVD)</h2>
        <p>Singular Value Decomposition (SVD) is a powerful technique used to decompose any matrix <code>A</code> into three matrices:</p>
        <pre><code>A = U Œ£ V<sup>T</sup></code></pre>
        <p>Where:</p>
        <ul>
            <li><code>U</code> is an <strong>orthogonal matrix</strong> (contains left singular vectors).</li>
            <li><code>Œ£</code> is a <strong>diagonal matrix</strong> (contains singular values).</li>
            <li><code>V<sup>T</sup></code> is an <strong>orthogonal matrix</strong> (contains right singular vectors).</li>
        </ul>
        <p>This decomposition is extremely useful in:</p>
        <ul>
            <li><strong>Dimensionality reduction (PCA in Machine Learning)</strong></li>
            <li><strong>Data compression (image and audio processing)</strong></li>
            <li><strong>Solving linear systems & least squares problems</strong></li>
            <li><strong>Noise reduction & signal processing</strong></li>
        </ul>
        <!-- Understanding the Components of SVD -->
<section id="svd-components">
    <h2>üü¢ Understanding the Components of SVD</h2>
    <p>Given an <strong>m √ó n</strong> matrix A, its SVD decomposition is:</p>
    <pre><code>A = U Œ£ V^T</code></pre>
    <p>Each component has a specific role:</p>

    <h3>1. Matrix U (Left Singular Vectors)</h3>
    <ul>
        <li><strong>Size:</strong> m √ó m</li>
        <li>Columns are <strong>orthonormal</strong> (U<sup>T</sup> U = I)</li>
        <li>Represents the <strong>directions of variance</strong> in the row space of A</li>
    </ul>

    <h3>2. Matrix Œ£ (Singular Values)</h3>
    <ul>
        <li><strong>Size:</strong> m √ó n (Only diagonal values are nonzero)</li>
        <li>Singular values <strong>œÉ‚ÇÅ, œÉ‚ÇÇ, ...</strong> are always <strong>non-negative</strong></li>
        <li>They represent <strong>how much each singular vector contributes</strong> to the transformation</li>
    </ul>

    <h3>3. Matrix V<sup>T</sup> (Right Singular Vectors)</h3>
    <ul>
        <li><strong>Size:</strong> n √ó n</li>
        <li>Columns are <strong>orthonormal</strong> (V<sup>T</sup> V = I)</li>
        <li>Represents the <strong>directions of variance</strong> in the column space of A</li>
    </ul>
</section>
<!-- Example of SVD -->
<section id="svd-example">
    <h2>üü¢ Example of Singular Value Decomposition</h2>
    <p>Let‚Äôs take a simple <strong>2 √ó 2</strong> matrix:</p>
    <pre><code>A = | 3  2 |
    | 2  3 |
    </code></pre>

    <p>1Ô∏è‚É£ <strong>Find U, Œ£, V<sup>T</sup></strong>:</p>
    <pre><code>U = |  0.707  -0.707 |
    |  0.707   0.707 |
        
Œ£ = | 5  0 |
    | 0  1 |

V<sup>T</sup> = |  0.707   0.707 |
    | -0.707   0.707 |
    </code></pre>

    <p>Now, we can reconstruct A using:</p>
    <pre><code>A = U Œ£ V<sup>T</sup></code></pre>

    <p><strong>Key Takeaway:</strong> The singular values <strong>(5 and 1)</strong> tell us how much of the original data is preserved in each principal direction.</p>
</section>
<!-- Why is A^T A Useful in SVD? -->
<section id="svd-gram-matrix">
    <h2>üü¢ Why is A<sup>T</sup> A Useful in SVD?</h2>
    <p>A key fact about <strong>SVD</strong> is that the <strong>singular values</strong> of A are related to the <strong>eigenvalues</strong> of A<sup>T</sup> A.</p>
    <ol>
        <li><strong>Compute eigenvalues of A<sup>T</sup> A:</strong>
            <pre><code>A<sup>T</sup> A = V Œ£¬≤ V<sup>T</sup></code></pre>
        </li>
        <li>The <strong>eigenvalues of A<sup>T</sup> A</strong> are the <strong>squared singular values</strong> of A.</li>
    </ol>

    <p>This connection helps <strong>find singular values efficiently</strong> and is used in <strong>Principal Component Analysis (PCA)</strong>.</p>
</section>
<!-- Applications of SVD -->
<section id="svd-applications">
    <h2>üü¢ Applications of SVD</h2>

    <h3>1. Data Compression</h3>
    <p>SVD allows us to <strong>approximate images</strong> by keeping only the largest singular values, reducing storage while preserving quality.</p>

    <h3>2. Principal Component Analysis (PCA)</h3>
    <p>PCA uses <strong>SVD</strong> to find <strong>principal components</strong> of a dataset, helping in <strong>dimensionality reduction</strong>.</p>

    <h3>3. Solving Linear Systems</h3>
    <p>When a system <strong>Ax = b</strong> has no unique solution, SVD helps find the <strong>best least-squares approximation</strong>.</p>

    <h3>4. Noise Reduction</h3>
    <p>Removing smaller singular values <strong>filters out noise</strong> in signals and images.</p>
</section>


    <!-- Key Code / Functions -->
    <section id="key-code">
        <h2>üü¢ Key Python Code</h2>
        <h3>Matrix Multiplication</h3>
        <pre><code>import numpy as np
A = np.array([[1, 2, 3], [4, 5, 6]])
B = np.array([[2, 0], [1, 3], [0, 4]])
C = np.dot(A, B)
print(C)</code></pre>

<h3>Computing SVD in Python</h3>
<pre><code>import numpy as np

A = np.array([[3, 2], [2, 3]])

# Compute SVD
U, S, Vt = np.linalg.svd(A)

print("U:\n", U)
print("Singular values:\n", S)
print("V^T:\n", Vt)</code></pre>

<h3>Image Compression Using SVD</h3>
<pre><code>import numpy as np
import matplotlib.pyplot as plt
from scipy.linalg import svd

# Load grayscale image
image = plt.imread("image.jpg")
gray = np.mean(image, axis=-1)

# Compute SVD
U, S, Vt = svd(gray)

# Keep only top 50 singular values
k = 50
compressed = U[:, :k] @ np.diag(S[:k]) @ Vt[:k, :]

# Show compressed image
plt.imshow(compressed, cmap="gray")
plt.title("Compressed Image with SVD")
plt.show()</code></pre>
</section>
    <script src="flashcards.js"></script>
</body>
</html>
