<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Session 6 - Bayesian Networks and Decision Trees</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <h1>Session 6 - Bayesian Networks and Decision Trees</h1>
    </header>
    
    <main>
        <a href="DAR-homepage.html" class="back-button">← Back to Homepage</a>
        <section id="objectives">
            <h2>Key Learning Objectives</h2>
            <ul>
                <li>Understand the fundamentals of Bayesian Networks and how they model probabilistic relationships.</li>
                <li>Learn the role of Bayes' Theorem in probabilistic classification and decision-making.</li>
                <li>Explore the Naïve Bayes classifier and its assumption of feature independence.</li>
                <li>Understand how Decision Trees make classification and regression predictions by splitting data.</li>
                <li>Learn about entropy, information gain, and Gini index as methods for decision tree splitting.</li>
                <li>Explore the impact of pruning on decision trees and how it prevents overfitting.</li>
            </ul>
        </section>
        
        
        <section id="flashcards">
            <h2>Flashcards</h2>
            <div class="flashcard-container">
                <div class="flashcard" data-question="What is Bayes' Theorem Formula?" data-answer="P(H|E) = (P(E|H) * P(H)) / P(E)"></div>
                <div class="flashcard" data-question="What assumption does Naïve Bayes make?" data-answer="Features are independent given the class label."></div>
                <div class="flashcard" data-question="What is a decision tree?" data-answer="A supervised learning algorithm that splits data based on feature conditions."></div>
                <div class="flashcard" data-question="What is pruning in decision trees?" data-answer="A technique to remove less significant branches to prevent overfitting."></div>
                <div class="flashcard" data-question="What is Bayes' Theorem?" data-answer="A formula that calculates the probability of an event given prior knowledge: P(H|E) = (P(E|H) * P(H)) / P(E)."></div>
                <div class="flashcard" data-question="What is a Bayesian Network?" data-answer="A graphical model that represents probabilistic relationships among variables using nodes and directed edges."></div>
                <div class="flashcard" data-question="What is the Naïve Bayes classifier?" data-answer="A classification algorithm based on Bayes' Theorem that assumes independence between predictors."></div>
                <div class="flashcard" data-question="What is the difference between classification and regression trees?" data-answer="Classification trees predict categorical outcomes / discrete labels, while regression trees predict continuous values."></div>
                <div class="flashcard" data-question="What is entropy in decision trees?" data-answer="A measure of randomness or impurity in a dataset used to determine the best split in a decision tree."></div>
                <div class="flashcard" data-question="What is information gain in decision trees?" data-answer="A metric that measures the reduction in entropy after a dataset is split based on a feature."></div>
                <div class="flashcard" data-question="Why is pruning used in decision trees?" data-answer="To reduce overfitting by removing branches that have little contribution to model accuracy."></div>
                <div class="flashcard" data-question="What are the advantages of decision trees?" data-answer="They are interpretable, handle categorical and numerical data, and require little preprocessing."></div>
                <div class="flashcard" data-question="What is the main disadvantage of decision trees?" data-answer="They are prone to overfitting, especially when deep trees are created without pruning."></div>
                <div class="flashcard" data-question="How does a Bayesian classifier make predictions?" data-answer="By calculating posterior probabilities for each class and assigning the class with the highest probability to the observation."></div>
            </div>
        </section>
        <section id="notes">
            <h2>Revision Notes</h2>
            <article>
                <h3>1. Bayesian Networks and Naïve Bayes Classifier</h3>
                <p><strong>Bayes’ Theorem:</strong> A fundamental theorem for probabilistic classification:</p>
                <pre>P(H|E) = (P(E|H) * P(H)) / P(E)</pre>
                <p>Where:</p>
                <ul>
                    <li>P(H|E): Posterior probability of hypothesis H given evidence E.</li>
                    <li>P(E|H): Likelihood of evidence given H.</li>
                    <li>P(H): Prior probability of H.</li>
                    <li>P(E): Total probability of evidence.</li>
                </ul>
                <p><strong>Naïve Bayes Assumption:</strong> Assumes features are independent given the class label.</p>
                
                <h3>2. Example: Predicting Play Golf Decision</h3>
                <p>Using Bayes' Theorem, we can classify whether a person will play golf based on weather conditions.</p>
                <p>Formula for classification:</p>
                <pre>P(Play Golf | Outlook) = (P(Outlook | Play Golf) * P(Play Golf)) / P(Outlook)</pre>
                
                <h3>3. Decision Trees</h3>
                <p><strong>What is a Decision Tree?</strong></p>
                <ul>
                    <li>Supervised learning algorithm for classification & regression.</li>
                    <li>Splits data based on feature conditions (e.g., Outlook = Sunny).</li>
                </ul>
                <p><strong>Types of Decision Trees:</strong></p>
                <ul>
                    <li>**Classification Tree:** Predicts discrete labels (e.g., Yes/No).</li>
                    <li>**Regression Tree:** Predicts continuous values.</li>
                </ul>
                
                <h3>4. Tree Construction and Pruning</h3>
                <p><strong>Tree Growth:</strong> Splitting nodes using impurity measures like Gini index or Entropy.</p>
                <p><strong>Pruning:</strong> Reduces tree complexity by removing less significant branches.</p>
                
                <h3>5. Advantages and Disadvantages of Decision Trees</h3>
                <p><strong>Advantages:</strong></p>
                <ul>
                    <li>Easy to interpret and visualize.</li>
                    <li>Handles both numerical and categorical data.</li>
                    <li>Non-parametric (no assumptions about data distribution).</li>
                </ul>
                <p><strong>Disadvantages:</strong></p>
                <ul>
                    <li>Prone to overfitting (can create deep, complex trees).</li>
                    <li>Greedy algorithm (may not find the global best split).</li>
                </ul>
            </article>
        </section>

        <section id="code-snippets">
            <h2>WEEK 6 LABS - R Code Snippets</h2>
        
            <h3>1. Loading and Exploring the Dataset</h3>
            <p>We begin by loading the <strong>Wine Quality Training Dataset</strong> to analyze factors affecting wine quality.</p>
            <pre><code>
        # Load training dataset
        training_data <- read.table("WineQuality_training.txt", header = TRUE, sep = ",")
        summary(training_data)
            </code></pre>
            <p><strong>Interpretation:</strong> This displays key statistics for the dataset, including mean, median, min/max, and quartiles.</p>
        
            <h3>2. Implementing 10-Fold Cross-Validation</h3>
            <p>We use <strong>10-fold cross-validation</strong> to evaluate model performance, ensuring our model generalizes well.</p>
            <pre><code>
        library(caret)
        library(ROCR)
        
        set.seed(11)  # Ensure reproducibility
        training_data[,3] <- as.factor(training_data[,3])  # Convert 'quality' to a factor
        folds <- createFolds(y=training_data[,3], k=10)  # Split data into 10 folds
            </code></pre>
            <p><strong>Interpretation:</strong> This partitions the dataset into 10 subsets, where 9 are used for training and 1 for validation.</p>
        
            <h3>3. Logistic Regression with Cross-Validation</h3>
            <p>We train a logistic regression model using <strong>alcohol content</strong> as the predictor variable.</p>
            <pre><code>
        auc_value_alcohol <- numeric()
        
        for(i in 1:10){
            fold_cv_test <- training_data[folds[[i]],]
            fold_cv_train <- training_data[-folds[[i]],]
            
            trained_model_alcohol <- glm(quality ~ alcohol, data = fold_cv_train, family = binomial)
            pred_prob_alcohol <- predict(trained_model_alcohol, fold_cv_test, type='response')
            
            pr_alcohol <- prediction(pred_prob_alcohol, fold_cv_test$quality)
            auroc_alcohol <- performance(pr_alcohol, measure = "auc")
            auc_value_alcohol <- append(auc_value_alcohol, auroc_alcohol@y.values[[1]])
        }
        
        print(mean(auc_value_alcohol))
            </code></pre>
            <p><strong>Interpretation:</strong> The <strong>mean AUROC score</strong> evaluates how well alcohol alone predicts wine quality (closer to 1 = better).</p>
        
            <h3>4. Comparing Predictive Power of Residual Sugar</h3>
            <p>We repeat cross-validation using <strong>residual sugar</strong> instead of alcohol.</p>
            <pre><code>
        auc_value_sugar <- numeric()
        
        for(i in 1:10){
            fold_cv_test <- training_data[folds[[i]],]
            fold_cv_train <- training_data[-folds[[i]],]
            
            trained_model_sugar <- glm(quality ~ residual.sugar, data = fold_cv_train, family = binomial)
            pred_prob_sugar <- predict(trained_model_sugar, fold_cv_test, type='response')
            
            pr_sugar <- prediction(pred_prob_sugar, fold_cv_test$quality)
            auroc_sugar <- performance(pr_sugar, measure = "auc")
            auc_value_sugar <- append(auc_value_sugar, auroc_sugar@y.values[[1]])
        }
        
        print(mean(auc_value_sugar))
            </code></pre>
            <p><strong>Interpretation:</strong> Compares how well <strong>residual sugar</strong> predicts quality compared to alcohol.</p>
        
            <h3>5. Combining Alcohol & Residual Sugar</h3>
            <p>We now train a model with both predictors to check for <strong>improved accuracy</strong>.</p>
            <pre><code>
        auc_value_alcohol_sugar <- numeric()
        
        for(i in 1:10){
            fold_cv_test <- training_data[folds[[i]],]
            fold_cv_train <- training_data[-folds[[i]],]
            
            trained_model_alcohol_sugar <- glm(quality ~ alcohol + residual.sugar, data = fold_cv_train, family = binomial)
            pred_prob_alcohol_sugar <- predict(trained_model_alcohol_sugar, fold_cv_test, type='response')
            
            pr_alcohol_sugar <- prediction(pred_prob_alcohol_sugar, fold_cv_test$quality)
            auroc_alcohol_sugar <- performance(pr_alcohol_sugar, measure = "auc")
            auc_value_alcohol_sugar <- append(auc_value_alcohol_sugar, auroc_alcohol_sugar@y.values[[1]])
        }
        
        print(mean(auc_value_alcohol_sugar))
            </code></pre>
            <p><strong>Interpretation:</strong> If this model's AUROC score is <strong>higher</strong> than previous models, it indicates alcohol and residual sugar <strong>together</strong> predict wine quality better.</p>
        
            <h3>6. Evaluating Model Accuracy on Test Data</h3>
            <p>We test the trained models on <strong>new data</strong> to verify accuracy.</p>
            <pre><code>
        testing_data <- read.table("WineQuality_testing.txt", header = TRUE, sep = ",")
        testing_data[,3] <- as.factor(testing_data[,3])
        
        prediction_trained_model_1 <- predict(trained_model_1, testing_data, type='response')
        prediction_trained_model_2 <- predict(trained_model_2, testing_data, type='response')
        prediction_trained_model_3 <- predict(trained_model_3, testing_data, type='response')
            </code></pre>
            <p><strong>Interpretation:</strong> We now compare model predictions to actual wine quality.</p>
        
            <h3>7. Calculating AUROC for Each Model</h3>
            <p>We calculate <strong>AUROC scores</strong> for each model.</p>
            <pre><code>
        pr_trained_model_1 <- prediction(prediction_trained_model_1, testing_data$quality)
        pr_trained_model_2 <- prediction(prediction_trained_model_2, testing_data$quality)
        pr_trained_model_3 <- prediction(prediction_trained_model_3, testing_data$quality)
        
        # Compute AUROC scores
        auroc_trained_model_1 <- performance(pr_trained_model_1, measure = "auc")@y.values[[1]]
        auroc_trained_model_2 <- performance(pr_trained_model_2, measure = "auc")@y.values[[1]]
        auroc_trained_model_3 <- performance(pr_trained_model_3, measure = "auc")@y.values[[1]]
        
        print(auroc_trained_model_1)  # AUROC for alcohol-only model
        print(auroc_trained_model_2)  # AUROC for sugar-only model
        print(auroc_trained_model_3)  # AUROC for combined model
            </code></pre>
            <p><strong>Interpretation:</strong> <strong>The highest AUROC score indicates the best predictive model</strong>.</p>
        </section>
        
    </main>
    
    <footer>
        <p>&copy; 2025 Thomas Baker - Data Analytics Revision</p>
    </footer>
    
    <script defer src="flashcards.js"></script>
</body>
</html>
