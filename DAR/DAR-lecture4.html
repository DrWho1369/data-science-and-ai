<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Session 4 - Logistic Regression</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <h1>Session 4 - Logistic Regression</h1>
    </header>
    
    <main>
        <a href="DAR-homepage.html" class="back-button">← Back to Homepage</a>
        <section id="objectives">
            <h2>Key Learning Objectives</h2>
            <ul>
                <li>Understand the difference between regression and classification problems.</li>
                <li>Learn how logistic regression models categorical outcomes.</li>
                <li>Explore the logistic function and its use in probability estimation.</li>
                <li>Understand Maximum Likelihood Estimation (MLE) for estimating logistic regression coefficients.</li>
                <li>Evaluate model performance using confusion matrices, accuracy, precision, recall, and AUC-ROC.</li>
            </ul>
        </section>

        <section id="flashcards">
            <h2>Flashcards</h2>
            <div class="flashcard-container">
                <div class="flashcard" data-question="What is classification?" data-answer="Predicting a categorical response variable."></div>
                <div class="flashcard" data-question="What is the formula for the logistic function?" data-answer="p(X) = e^(β0+β1X) / (1 + e^(β0+β1X))"></div>
                <div class="flashcard" data-question="What does a positive β1 coefficient mean?" data-answer="As X increases, probability of Y=1 increases."></div>
                <div class="flashcard" data-question="How is model performance evaluated?" data-answer="Using a confusion matrix, accuracy, precision, recall, and F1-score."></div>
                <div class="flashcard" data-question="What is the key difference between regression and classification?" data-answer="Regression predicts continuous outcomes, while classification predicts categorical outcomes."></div>
                <div class="flashcard" data-question="Why is linear regression not suitable for classification?" data-answer="Linear regression can predict values beyond the range of 0 and 1, making it inappropriate for probability estimation."></div>
                <div class="flashcard" data-question="What function is used in logistic regression to constrain outputs between 0 and 1?" data-answer="The logistic (sigmoid) function, defined as p(X) = e^(β0+β1X) / (1 + e^(β0+β1X))."></div>
                <div class="flashcard" data-question="What is the log-odds (logit) transformation in logistic regression?" data-answer="It is the logarithm of the odds ratio: log(p(X) / (1 - p(X))) = β0 + β1X."></div>
                <div class="flashcard" data-question="What does the coefficient β1 indicate in logistic regression?" data-answer="β1 represents the change in log-odds for a one-unit increase in X, or equivalently, e^β1 is the odds multiplier."></div>
                <div class="flashcard" data-question="What statistical method is used to estimate coefficients in logistic regression?" data-answer="Maximum Likelihood Estimation (MLE) is used to find the parameters that maximize the likelihood of observed data."></div>
                <div class="flashcard" data-question="What is the decision rule for classification using logistic regression?" data-answer="If P(Y=1 | X) > 0.5, predict class 1; otherwise, predict class 0."></div>
                <div class="flashcard" data-question="How do you evaluate a logistic regression model?" data-answer="Using a confusion matrix, accuracy, precision, recall, F1-score, and the Area Under the ROC Curve (AUC)."></div>
                <div class="flashcard" data-question="What is multiple logistic regression?" data-answer="A logistic regression model that includes multiple predictor variables instead of just one."></div>
                <div class="flashcard" data-question="What is the purpose of an ROC curve?" data-answer="To visualize the trade-off between sensitivity (true positive rate) and specificity (false positive rate) at different threshold values."></div>
            </div>
        </section>

        <section id="notes">
            <h2>Revision Notes</h2>
            <article>
                <h3>1. Introduction to Classification</h3>
                <p><strong>Classification:</strong> Predicting a categorical response variable.</p>
                <p><strong>Regression vs. Classification:</strong> Regression predicts continuous values, while classification predicts discrete categories.</p>
                <p><strong>Common Classification Algorithms:</strong> Logistic Regression, Decision Trees, Naïve Bayes, SVM, Neural Networks.</p>
                
                <h3>2. Why Not Linear Regression?</h3>
                <p>Linear regression is not suitable for classification because:</p>
                <ul>
                    <li>Predicted values can be outside the 0-1 probability range.</li>
                    <li>Does not work well for categorical outcomes.</li>
                </ul>
                
                <h3>3. Logistic Regression & The Logistic Function</h3>
                <p><strong>Logistic Function:</strong> Ensures output values are between 0 and 1.</p>
                <pre>
p(X) = e^(β0+β1X) / (1 + e^(β0+β1X))
                </pre>
                <p><strong>Log-Odds (Logit):</strong> Taking the log of odds gives a linear relationship:</p>
                <pre>
log(p(X) / (1 - p(X))) = β0 + β1X
                </pre>
                
                <h3>4. Estimating Coefficients: Maximum Likelihood</h3>
                <p>Instead of least squares, logistic regression uses <strong>Maximum Likelihood Estimation (MLE)</strong> to find the best-fitting parameters.</p>
                
                <h3>5. Interpreting Coefficients</h3>
                <ul>
                    <li><strong>β1 &gt; 0:</strong> As X increases, probability of Y=1 increases.</li>
                    <li><strong>β1 &lt; 0:</strong> As X increases, probability of Y=1 decreases.</li>
                    <li><strong>Odds Ratio:</strong> e^(β1) represents how the odds change with a unit increase in X.</li>
                </ul>
                
                <h3>6. Making Predictions</h3>
                <p>Predicted probability: If P(Y=1 | X) > 0.5, classify as Y=1; otherwise, Y=0.</p>
                
                <h3>7. Evaluating Logistic Regression Models</h3>
                <ul>
                    <li><strong>Confusion Matrix:</strong> Shows True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN).</li>
                    <li><strong>Accuracy:</strong> (TP + TN) / Total.</li>
                    <li><strong>Precision, Recall, F1-score:</strong> Metrics for performance evaluation.</li>
                </ul>
            </article>
        </section>

        <section id="code-snippets">
            <h2>WEEK 4 LABS - R Code Snippets</h2>
        
            <h3>1. Loading Data & Basic Exploration</h3>
            <p>We begin by loading the dataset and installing required packages:</p>
            <pre><code>
        # Install package if not already installed
        install.packages("aod")
        
        # Load necessary library
        library(aod)
        
        # Read dataset from URL
        mydata <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")
        
        # Display the first few rows
        head(mydata)
            </code></pre>
        
            <p><strong>Interpretation:</strong> This loads a dataset of student admissions, containing GRE, GPA, and rank information.</p>
        
            <h3>2. Descriptive Statistics</h3>
            <p>To get a quick summary of the dataset:</p>
            <pre><code>
        summary(mydata)
            </code></pre>
        
            <p><strong>Interpretation:</strong> This provides key statistics (mean, median, min/max) for each variable.</p>
        
            <h3>3. Checking Number of Observations</h3>
            <p>We count the number of rows (observations) in the dataset:</p>
            <pre><code>
        nrow(mydata)
            </code></pre>
        
            <p><strong>Interpretation:</strong> This dataset contains 400 observations.</p>
        
            <h3>4. Standard Deviation & Mean Calculations</h3>
            <p>We calculate standard deviation and mean for numerical variables:</p>
            <pre><code>
        sapply(mydata, sd)  # Standard deviations
        sapply(mydata[, -4], mean)  # Mean for numerical variables (excluding rank)
            </code></pre>
        
            <p><strong>Interpretation:</strong> Standard deviation shows variability, while mean gives the average values for GRE, GPA, and admission.</p>
        
            <h3>5. Converting Rank to a Categorical Variable</h3>
            <p>Since rank is an ordinal variable, we convert it to a factor:</p>
            <pre><code>
        mydata$rank <- factor(mydata$rank)
            </code></pre>
        
            <p><strong>Interpretation:</strong> This ensures that the rank variable is treated correctly in regression models.</p>
        
            <h3>6. Logistic Regression Model</h3>
            <p>We estimate a logistic regression model predicting admission:</p>
            <pre><code>
        glm.admit.fit <- glm(admit ~ gre + gpa + rank, data = mydata, family = "binomial")
        summary(glm.admit.fit)
            </code></pre>
        
            <p><strong>Interpretation:</strong> The model estimates the probability of admission based on GRE, GPA, and university ranking.</p>
        
            <h3>7. Interpreting Categorical Variables in Regression</h3>
            <p>Rank is replaced by binary variables:</p>
            <ul>
                <li><strong>Rank2</strong>: 1 if rank is 2, otherwise 0.</li>
                <li><strong>Rank3</strong>: 1 if rank is 3, otherwise 0.</li>
                <li><strong>Rank4</strong>: 1 if rank is 4, otherwise 0.</li>
            </ul>
        
            <p>Rank1 is omitted as a reference category.</p>
        
            <h3>8. Making Predictions</h3>
            <p>We predict admission probabilities using the logistic regression model:</p>
            <pre><code>
        admit.prob <- predict(glm.admit.fit, type = "response")
        head(admit.prob)
            </code></pre>
        
            <p><strong>Interpretation:</strong> The model outputs the probability of admission for each student.</p>
        
            <h3>9. Creating Binary Predictions</h3>
            <p>We convert predicted probabilities into binary outcomes:</p>
            <pre><code>
        admit.pred <- rep(1, 400)
        admit.pred[admit.prob < 0.5] <- 0
        head(admit.pred)
            </code></pre>
        
            <p><strong>Interpretation:</strong> Admissions are predicted as 1 if probability ≥ 0.5, otherwise 0.</p>
        
            <h3>10. Confusion Matrix & Accuracy</h3>
            <p>We evaluate model performance using a confusion matrix:</p>
            <pre><code>
        table(admit.pred, mydata$admit)  # Confusion matrix
        mean(admit.pred == mydata$admit)  # Accuracy
            </code></pre>
        
            <p><strong>Interpretation:</strong> The confusion matrix shows correctly and incorrectly classified admissions, while accuracy gives the percentage of correct predictions.</p>
        
            <h3>11. Predicting Admissions for Average Students</h3>
            <p>We predict admission probabilities for students with average GRE & GPA:</p>
            <pre><code>
        newdata1 <- data.frame(gre = mean(mydata$gre), gpa = mean(mydata$gpa), rank = factor(1:4))
        newdata1$admit1.prob <- predict(glm.admit.fit, newdata = newdata1, type = "response")
        newdata1
            </code></pre>
        
            <p><strong>Interpretation:</strong> This shows how admission probability changes based on university rank.</p>
        </section>
        
    </main>
    
    <footer>
        <p>&copy; 2025 Thomas Baker - MSc Data Science & AI Revision</p>
    </footer>
    
    <script defer src="flashcards.js"></script>
</body>
</html>
