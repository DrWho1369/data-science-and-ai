<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Session 7 - Bagging and Random Forests</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <h1>Session 7 - Bagging and Random Forests</h1>
    </header>
    
    <main>
        <a href="DAR-homepage.html" class="back-button">← Back to Homepage</a>
        <section id="objectives">
            <h2>Key Learning Objectives</h2>
            <ul>
                <li>Understand the limitations of decision trees and how bagging reduces variance.</li>
                <li>Learn how bootstrapping works and why it is used in bagging.</li>
                <li>Explore how bagging improves model stability by averaging predictions.</li>
                <li>Understand how Random Forests improve upon bagging by introducing feature randomness.</li>
                <li>Learn how Out-of-Bag (OOB) error is used to estimate model performance.</li>
                <li>Understand the concept of Variable Importance and how Random Forests measure it.</li>
            </ul>
        </section>
        
        <section id="flashcards">
            <h2>Flashcards</h2>
            <div class="flashcard-container">
                <div class="flashcard" data-question="What is the purpose of Bagging?" data-answer="To reduce variance in models by averaging predictions."></div>
                <div class="flashcard" data-question="How does Bootstrapping work?" data-answer="It randomly samples data with replacement to create multiple training sets."></div>
                <div class="flashcard" data-question="What is the main advantage of Random Forests over Bagging?" data-answer="Random forests decorrelate trees by selecting a random subset of features at each split."></div>
                <div class="flashcard" data-question="How is the Out-of-Bag (OOB) error calculated?" data-answer="By evaluating model performance on samples not used in training."></div>
                <div class="flashcard" data-question="What is Variable Importance in Random Forests?" data-answer="A measure of how much a feature contributes to model accuracy."></div>
                <div class="flashcard" data-question="What is the main disadvantage of decision trees?" data-answer="They suffer from high variance, meaning small changes in data can lead to different tree structures."></div>
                <div class="flashcard" data-question="How does bagging improve decision trees?" data-answer="Bagging reduces variance by averaging predictions from multiple decision trees trained on bootstrapped datasets."></div>
                <div class="flashcard" data-question="What is bootstrapping in bagging?" data-answer="A resampling technique where training data is randomly sampled with replacement to create multiple datasets."></div>
                <div class="flashcard" data-question="How does bagging work for regression and classification?" data-answer="For regression, it averages predictions; for classification, it takes a majority vote among models."></div>
                <div class="flashcard" data-question="What is Out-of-Bag (OOB) error estimation?" data-answer="An estimate of model error calculated using the unused (out-of-bag) samples from bootstrapped datasets."></div>
                <div class="flashcard" data-question="What is the key difference between bagging and random forests?" data-answer="Random forests improve bagging by decorrelating trees, randomly selecting a subset of predictors at each split."></div>
                <div class="flashcard" data-question="Why does random forests reduce overfitting compared to bagging?" data-answer="By using only a random subset of features at each split, it prevents dominant variables from always being selected."></div>
                <div class="flashcard" data-question="How is variable importance measured in random forests?" data-answer="By calculating the decrease in Mean Squared Error (MSE) or Gini impurity when a variable is used for splitting."></div>
                <div class="flashcard" data-question="What is the role of the ‘mtry’ parameter in random forests?" data-answer="It controls the number of predictors randomly selected at each split, improving diversity between trees."></div>
                <div class="flashcard" data-question="How does increasing the number of trees in a random forest affect performance?" data-answer="It stabilizes predictions and reduces variance, but after a point, adding more trees does not significantly improve accuracy."></div>
            </div>
        </section>
        <section id="notes">
            <h2>Revision Notes</h2>
            <article>
                <h3>1. Introduction to Bagging</h3>
                <p><strong>Problem with Decision Trees:</strong> High variance in predictions.</p>
                <p><strong>Bagging (Bootstrap Aggregating):</strong> Combines multiple decision trees trained on bootstrapped datasets.</p>
                <ul>
                    <li>Reduces variance by averaging predictions.</li>
                    <li>Uses random sampling with replacement.</li>
                </ul>
                
                <h3>2. Bootstrapping</h3>
                <p><strong>Random Sampling with Replacement:</strong> Each bootstrap dataset is sampled from the original dataset.</p>
                <p><strong>Key Idea:</strong> Repeated sampling improves model stability.</p>
                
                <h3>3. Bagging for Regression & Classification Trees</h3>
                <ul>
                    <li><strong>Regression:</strong> Average predictions of all trees.</li>
                    <li><strong>Classification:</strong> Majority vote across all trees.</li>
                </ul>
                
                <h3>4. Random Forests</h3>
                <p>Enhances bagging by introducing <strong>random feature selection</strong>.</p>
                <ul>
                    <li>Each tree selects a random subset of features at each split.</li>
                    <li>Helps decorrelate trees, improving model robustness.</li>
                </ul>
                
                <h3>5. Evaluating Model Performance</h3>
                <p><strong>Out-of-Bag (OOB) Error:</strong> Uses unseen samples for validation.</p>
                <p><strong>Variable Importance:</strong> Measures the impact of each feature.</p>
                
                <h3>6. Example: Boston Housing Data</h3>
                <pre>
                library(randomForest)
                set.seed(1)
                bag.boston <- randomForest(medv ~ ., data=Boston, mtry=12, importance=TRUE)
                yhat.bag <- predict(bag.boston, newdata=Boston[-train,])
                mean((yhat.bag - Boston[-train, "medv"])^2)
                </pre>
            </article>
        </section>

        <section id="code-snippets">
            <h2>WEEK 7 LABS - R Code Snippets</h2>
        
            <h3>1. Loading and Exploring the Dataset</h3>
            <p>We begin by loading the <strong>Wine Quality Training Dataset</strong> to analyze factors affecting wine quality.</p>
            <pre><code>
        # Load training dataset
        training_data <- read.table("WineQuality_training.txt", header = TRUE, sep = ",")
        summary(training_data)
            </code></pre>
            <p><strong>Interpretation:</strong> This displays key statistics for the dataset, including mean, median, min/max, and quartiles.</p>
        
            <h3>2. Implementing 10-Fold Cross-Validation</h3>
            <p>We use <strong>10-fold cross-validation</strong> to evaluate model performance, ensuring our model generalizes well.</p>
            <pre><code>
        library(caret)
        library(ROCR)
        
        set.seed(11)  # Ensure reproducibility
        training_data[,3] <- as.factor(training_data[,3])  # Convert 'quality' to a factor
        folds <- createFolds(y=training_data[,3], k=10)  # Split data into 10 folds
            </code></pre>
            <p><strong>Interpretation:</strong> This partitions the dataset into 10 subsets, where 9 are used for training and 1 for validation.</p>
        
            <h3>3. Logistic Regression with Cross-Validation</h3>
            <p>We train a logistic regression model using <strong>alcohol content</strong> as the predictor variable.</p>
            <pre><code>
        auc_value_alcohol <- numeric()
        
        for(i in 1:10){
            fold_cv_test <- training_data[folds[[i]],]
            fold_cv_train <- training_data[-folds[[i]],]
            
            trained_model_alcohol <- glm(quality ~ alcohol, data = fold_cv_train, family = binomial)
            pred_prob_alcohol <- predict(trained_model_alcohol, fold_cv_test, type='response')
            
            pr_alcohol <- prediction(pred_prob_alcohol, fold_cv_test$quality)
            auroc_alcohol <- performance(pr_alcohol, measure = "auc")
            auc_value_alcohol <- append(auc_value_alcohol, auroc_alcohol@y.values[[1]])
        }
        
        print(mean(auc_value_alcohol))
            </code></pre>
            <p><strong>Interpretation:</strong> The <strong>mean AUROC score</strong> evaluates how well alcohol alone predicts wine quality (closer to 1 = better).</p>
        
            <h3>4. Comparing Predictive Power of Residual Sugar</h3>
            <p>We repeat cross-validation using <strong>residual sugar</strong> instead of alcohol.</p>
            <pre><code>
        auc_value_sugar <- numeric()
        
        for(i in 1:10){
            fold_cv_test <- training_data[folds[[i]],]
            fold_cv_train <- training_data[-folds[[i]],]
            
            trained_model_sugar <- glm(quality ~ residual.sugar, data = fold_cv_train, family = binomial)
            pred_prob_sugar <- predict(trained_model_sugar, fold_cv_test, type='response')
            
            pr_sugar <- prediction(pred_prob_sugar, fold_cv_test$quality)
            auroc_sugar <- performance(pr_sugar, measure = "auc")
            auc_value_sugar <- append(auc_value_sugar, auroc_sugar@y.values[[1]])
        }
        
        print(mean(auc_value_sugar))
            </code></pre>
            <p><strong>Interpretation:</strong> Compares how well <strong>residual sugar</strong> predicts quality compared to alcohol.</p>
        
            <h3>5. Combining Alcohol & Residual Sugar</h3>
            <p>We now train a model with both predictors to check for <strong>improved accuracy</strong>.</p>
            <pre><code>
        auc_value_alcohol_sugar <- numeric()
        
        for(i in 1:10){
            fold_cv_test <- training_data[folds[[i]],]
            fold_cv_train <- training_data[-folds[[i]],]
            
            trained_model_alcohol_sugar <- glm(quality ~ alcohol + residual.sugar, data = fold_cv_train, family = binomial)
            pred_prob_alcohol_sugar <- predict(trained_model_alcohol_sugar, fold_cv_test, type='response')
            
            pr_alcohol_sugar <- prediction(pred_prob_alcohol_sugar, fold_cv_test$quality)
            auroc_alcohol_sugar <- performance(pr_alcohol_sugar, measure = "auc")
            auc_value_alcohol_sugar <- append(auc_value_alcohol_sugar, auroc_alcohol_sugar@y.values[[1]])
        }
        
        print(mean(auc_value_alcohol_sugar))
            </code></pre>
            <p><strong>Interpretation:</strong> If this model's AUROC score is <strong>higher</strong> than previous models, it indicates alcohol and residual sugar <strong>together</strong> predict wine quality better.</p>
        
            <h3>6. Evaluating Model Accuracy on Test Data</h3>
            <p>We test the trained models on <strong>new data</strong> to verify accuracy.</p>
            <pre><code>
        testing_data <- read.table("WineQuality_testing.txt", header = TRUE, sep = ",")
        testing_data[,3] <- as.factor(testing_data[,3])
        
        prediction_trained_model_1 <- predict(trained_model_1, testing_data, type='response')
        prediction_trained_model_2 <- predict(trained_model_2, testing_data, type='response')
        prediction_trained_model_3 <- predict(trained_model_3, testing_data, type='response')
            </code></pre>
            <p><strong>Interpretation:</strong> We now compare model predictions to actual wine quality.</p>
        
            <h3>7. Calculating AUROC for Each Model</h3>
            <p>We calculate <strong>AUROC scores</strong> for each model.</p>
            <pre><code>
        pr_trained_model_1 <- prediction(prediction_trained_model_1, testing_data$quality)
        pr_trained_model_2 <- prediction(prediction_trained_model_2, testing_data$quality)
        pr_trained_model_3 <- prediction(prediction_trained_model_3, testing_data$quality)
        
        # Compute AUROC scores
        auroc_trained_model_1 <- performance(pr_trained_model_1, measure = "auc")@y.values[[1]]
        auroc_trained_model_2 <- performance(pr_trained_model_2, measure = "auc")@y.values[[1]]
        auroc_trained_model_3 <- performance(pr_trained_model_3, measure = "auc")@y.values[[1]]
        
        print(auroc_trained_model_1)  # AUROC for alcohol-only model
        print(auroc_trained_model_2)  # AUROC for sugar-only model
        print(auroc_trained_model_3)  # AUROC for combined model
            </code></pre>
            <p><strong>Interpretation:</strong> <strong>The highest AUROC score indicates the best predictive model</strong>.</p>
        </section>
        
    </main>
    
    <footer>
        <p>&copy; 2025 Thomas Baker - Data Analytics Revision</p>
    </footer>
    
    <script defer src="flashcards.js"></script>
</body>
</html>
