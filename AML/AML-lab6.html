<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AML Lab 6 - Deep Learning with Keras</title>
    <link rel="stylesheet" href="aml-styles.css">
</head>
<body>
    <header>
        <h1>AML Lab 6 - Deep Learning with Keras</h1>
    </header>
    
    <main>
        <a href="AML-homepage.html" class="back-button">‚Üê Back to Homepage</a>

        <section id="lab6">
            <h2>ü§ñ Lab 6: Deep Learning with Keras</h2>
            <p>Deep learning is a subset of machine learning that mimics the human brain by using **neural networks**. In this lab, we will implement a **Multi-Layer Perceptron (MLP)** using **Keras** with **TensorFlow** as the backend. We will cover:</p>
            <ul>
                <li>Defining an MLP model using **Sequential API and Functional API**.</li>
                <li>Compiling the model by selecting an **optimizer and loss function**.</li>
                <li>Training the model and monitoring its **performance over time**.</li>
                <li>Evaluating the model and understanding **generalization and overfitting**.</li>
            </ul>
        
            <h3>üìå Why Keras?</h3>
            <p><strong>Keras</strong> is a high-level neural network API that simplifies deep learning development while utilizing powerful backend engines like **TensorFlow**. Benefits of Keras:</p>
            <ul>
                <li>**User-Friendly** ‚Äì Simplifies neural network implementation.</li>
                <li>**Fast Prototyping** ‚Äì Quick model development and experimentation.</li>
                <li>**Built-in Support** ‚Äì Pre-trained models and advanced layers are readily available.</li>
            </ul>
        
            <h3>üìå Defining an MLP Model with Keras</h3>
            <p>The **Sequential API** allows us to build deep learning models **layer by layer**.</p>
            <pre><code>
from keras.models import Sequential
from keras.layers import Dense
import numpy

# Set random seed for reproducibility
numpy.random.seed(7)

# Define a simple feedforward MLP model
model = Sequential()
model.add(Dense(12, input_dim=8, activation='relu'))  # First hidden layer
model.add(Dense(8, activation='relu'))  # Second hidden layer
model.add(Dense(1, activation='sigmoid'))  # Output layer
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li><strong>Hidden Layers:</strong> The model consists of <strong>two hidden layers</strong> with **ReLU activation**.</li>
                <li><strong>Input Dimension:</strong> <code>input_dim=8</code> means the model takes **8 input features** from the dataset.</li>
                <li><strong>Activation Functions:</strong>
                    <ul>
                        <li><strong>ReLU (Rectified Linear Unit):</strong> Used in hidden layers to introduce **non-linearity**, helping the model learn complex patterns.</li>
                        <li><strong>Sigmoid:</strong> Used in the output layer since this is a **binary classification** problem (0 or 1).</li>
                    </ul>
                </li>
            </ul>
        
            <h3>üìå Alternative: Functional API</h3>
            <p>The Functional API is more flexible and allows **complex model architectures** such as shared layers and multiple inputs/outputs.</p>
            <pre><code>
from keras import Input, Model

input_layer = Input(shape=(8,))
hidden_layer1 = Dense(12, activation='relu')(input_layer)
hidden_layer2 = Dense(8, activation='relu')(hidden_layer1)
output_layer = Dense(1, activation='sigmoid')(hidden_layer2)

model = Model(inputs=input_layer, outputs=output_layer)
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>Allows for **custom architectures**, useful for complex models such as **residual networks**.</li>
                <li>Each layer **connects explicitly**, giving more control over the network structure.</li>
            </ul>
        
            <h3>üìå Model Compilation</h3>
            <p>Before training, we must **compile the model**, selecting the **loss function**, **optimizer**, and **evaluation metric**.</p>
            <pre><code>
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li><strong>Loss Function:</strong> <code>binary_crossentropy</code> is used for **binary classification problems**.</li>
                <li><strong>Optimizer:</strong> <code>adam</code> (Adaptive Moment Estimation) dynamically adjusts the learning rate for better convergence.</li>
                <li><strong>Evaluation Metric:</strong> <code>accuracy</code> tracks how often predictions match actual values.</li>
            </ul>
        
            <h3>üìå Training the Model</h3>
            <p>The **fit() function** trains the model for a specified number of **epochs**.</p>
            <pre><code>
model.fit(X, Y, epochs=150, batch_size=10)
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>The model **trains for 150 epochs**, meaning it passes through the dataset 150 times.</li>
                <li>A **batch size of 10** means weights are updated after every 10 samples.</li>
                <li>Smaller batch sizes lead to **smoother convergence**, while larger batches speed up training but might cause instability.</li>
            </ul>
        
            <h3>üìå Model Evaluation</h3>
            <p>After training, we evaluate the model on **unseen data** to check its generalization ability.</p>
            <pre><code>
scores = model.evaluate(X, Y)
print(f"Accuracy: {scores[1]*100:.2f}%")
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>Evaluates the model‚Äôs performance on **test data**.</li>
                <li>Accuracy score shows **how well the model performs**.</li>
                <li>Using the **same training data for evaluation** leads to overfitting‚Äîseparate test sets should be used.</li>
            </ul>
        
            <h3>üìå Summary</h3>
            <ul>
                <li>Built an <strong>MLP model</strong> using **Keras Sequential and Functional API**.</li>
                <li>Compiled the model using **loss functions, optimizers, and metrics**.</li>
                <li>Trained the model for **multiple epochs** with **batch processing**.</li>
                <li>Evaluated model performance using **accuracy as a metric**.</li>
            </ul>
        </section>
    </main>
    
    <footer>
        <p>&copy; 2025 Thomas Baker - MSc Data Science & AI Revision</p>
    </footer>
    
    <script defer src="aml-flashcards.js"></script>
</body>
</html>