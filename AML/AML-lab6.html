<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AML Lab 6 - Deep Learning with Keras</title>
    <link rel="stylesheet" href="aml-styles.css">
</head>
<body>
    <header>
        <h1>AML Lab 6 - Deep Learning with Keras</h1>
    </header>
    
    <main>
        <a href="AML-homepage.html" class="back-button">‚Üê Back to Homepage</a>

        <section id="lab6">
            <h2>ü§ñ Lab 6: Deep Learning with Keras</h2>
            <p>Deep learning is a subset of machine learning that mimics the human brain by using <strong>neural networks</strong>. In this lab, we will implement a <strong>Multi-Layer Perceptron (MLP)</strong> using <strong>Keras</strong> with <strong>TensorFlow</strong> as the backend. We will cover:</p>
            <ul>
                <li>Defining an MLP model using <strong>Sequential API and Functional API</strong>.</li>
                <li>Compiling the model by selecting an <strong>optimizer and loss function</strong>.</li>
                <li>Training the model and monitoring its <strong>performance over time</strong>.</li>
                <li>Evaluating the model and understanding <strong>generalization and overfitting</strong>.</li>
            </ul>
        
            <h3>üìå Why Keras?</h3>
            <p><strong>Keras</strong> is a high-level neural network API that simplifies deep learning development while utilizing powerful backend engines like <strong>TensorFlow</strong>. Benefits of Keras:</p>
            <ul>
                <li><strong>User-Friendly</strong> ‚Äì Simplifies neural network implementation.</li>
                <li><strong>Fast Prototyping</strong> ‚Äì Quick model development and experimentation.</li>
                <li><strong>Built-in Support</strong> ‚Äì Pre-trained models and advanced layers are readily available.</li>
            </ul>
        
            <h3>üìå Defining an MLP Model with Keras</h3>
            <p>The <strong>Sequential API</strong> allows us to build deep learning models <strong>layer by layer</strong>.</p>
            <pre><code>
from keras.models import Sequential
from keras.layers import Dense
import numpy

# Set random seed for reproducibility
numpy.random.seed(7)

# Define a simple feedforward MLP model
model = Sequential()
model.add(Dense(12, input_dim=8, activation='relu'))  # First hidden layer
model.add(Dense(8, activation='relu'))  # Second hidden layer
model.add(Dense(1, activation='sigmoid'))  # Output layer
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li><strong>Hidden Layers:</strong> The model consists of <strong>two hidden layers</strong> with <strong>ReLU activation</strong>.</li>
                <li><strong>Input Dimension:</strong> <code>input_dim=8</code> means the model takes <strong>8 input features</strong> from the dataset.</li>
                <li><strong>Activation Functions:</strong>
                    <ul>
                        <li><strong>ReLU (Rectified Linear Unit):</strong> Used in hidden layers to introduce <strong>non-linearity</strong>, helping the model learn complex patterns.</li>
                        <li><strong>Sigmoid:</strong> Used in the output layer since this is a <strong>binary classification</strong> problem (0 or 1).</li>
                    </ul>
                </li>
            </ul>
        
            <h3>üìå Alternative: Functional API</h3>
            <p>The Functional API is more flexible and allows <strong>complex model architectures</strong> such as shared layers and multiple inputs/outputs.</p>
            <pre><code>
from keras import Input, Model

input_layer = Input(shape=(8,))
hidden_layer1 = Dense(12, activation='relu')(input_layer)
hidden_layer2 = Dense(8, activation='relu')(hidden_layer1)
output_layer = Dense(1, activation='sigmoid')(hidden_layer2)

model = Model(inputs=input_layer, outputs=output_layer)
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>Allows for <strong>custom architectures</strong>, useful for complex models such as <strong>residual networks</strong>.</li>
                <li>Each layer <strong>connects explicitly</strong>, giving more control over the network structure.</li>
            </ul>
        
            <h3>üìå Model Compilation</h3>
            <p>Before training, we must <strong>compile the model</strong>, selecting the <strong>loss function</strong>, <strong>optimizer</strong>, and <strong>evaluation metric</strong>.</p>
            <pre><code>
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li><strong>Loss Function:</strong> <code>binary_crossentropy</code> is used for <strong>binary classification problems</strong>.</li>
                <li><strong>Optimizer:</strong> <code>adam</code> (Adaptive Moment Estimation) dynamically adjusts the learning rate for better convergence.</li>
                <li><strong>Evaluation Metric:</strong> <code>accuracy</code> tracks how often predictions match actual values.</li>
            </ul>
        
            <h3>üìå Training the Model</h3>
            <p>The <strong>fit() function</strong> trains the model for a specified number of <strong>epochs</strong>.</p>
            <pre><code>
model.fit(X, Y, epochs=150, batch_size=10)
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>The model <strong>trains for 150 epochs</strong>, meaning it passes through the dataset 150 times.</li>
                <li>A <strong>batch size of 10</strong> means weights are updated after every 10 samples.</li>
                <li>Smaller batch sizes lead to <strong>smoother convergence</strong>, while larger batches speed up training but might cause instability.</li>
            </ul>
        
            <h3>üìå Model Evaluation</h3>
            <p>After training, we evaluate the model on <strong>unseen data</strong> to check its generalization ability.</p>
            <pre><code>
scores = model.evaluate(X, Y)
print(f"Accuracy: {scores[1]*100:.2f}%")
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>Evaluates the model‚Äôs performance on <strong>test data</strong>.</li>
                <li>Accuracy score shows <strong>how well the model performs</strong>.</li>
                <li>Using the <strong>same training data for evaluation</strong> leads to overfitting‚Äîseparate test sets should be used.</li>
            </ul>
        
            <h3>üìå Summary</h3>
            <ul>
                <li>Built an <strong>MLP model</strong> using <strong>Keras Sequential and Functional API</strong>.</li>
                <li>Compiled the model using <strong>loss functions, optimizers, and metrics</strong>.</li>
                <li>Trained the model for <strong>multiple epochs</strong> with <strong>batch processing</strong>.</li>
                <li>Evaluated model performance using <strong>accuracy as a metric</strong>.</li>
            </ul>
        </section>
    </main>
    
    <footer>
        <p>&copy; 2025 Thomas Baker - MSc Data Science & AI Revision</p>
    </footer>
    
    <script defer src="aml-flashcards.js"></script>
</body>
</html>