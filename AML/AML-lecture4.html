<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Session 4 - Linear Regression & Neural Networks</title>
    <link rel="stylesheet" href="aml-styles.css">
</head>
<body>
    <header class="hero">
        <h1>Session 4 - Linear Regression & Neural Networks</h1>
    </header>
    
    <main>
        <a href="AML-homepage.html" class="back-button">‚Üê Back to Homepage</a>

        <section id="objectives">
            <h2>Key Learning Objectives</h2>
            <ul>
                <li>Understand the difference between parametric models (Linear Regression) and non-parametric models (Neural Networks).</li>
                <li>Learn how Linear Regression works and how it models relationships between variables.</li>
                <li>Understand the logistic function and its role in classification.</li>
                <li>Explore the fundamentals of Neural Networks, including architecture, activation functions, and weight updates.</li>
                <li>Learn about different activation functions (Sigmoid, TanH, ReLU) and their impact on model performance.</li>
                <li>Understand the importance of cost functions and how different loss functions (MSE, MAE, Cross-Entropy) are used.</li>
                <li>Explore network regularization techniques, including Dropout and Early Stopping.</li>
            </ul>
        </section>        
        
        <section id="flashcards">
            <h2>Flashcards</h2>
            <div class="flashcard-container">
                <div class="flashcard" data-question="What is the key assumption of Linear Regression?" data-answer="Linear Regression assumes a linear relationship between input and output variables."></div>
                <div class="flashcard" data-question="What is the equation for Linear Regression?" data-answer="≈∑ = w0 + w1x1 + w2x2 + ... + wn*xn, where w are weights."></div>
                <div class="flashcard" data-question="What does the sigmoid activation function do?" data-answer="It transforms inputs into a range between 0 and 1, useful for probability-based classification."></div>
                <div class="flashcard" data-question="Why is the TanH activation function often preferred over Sigmoid?" data-answer="TanH is centered around zero (-1 to 1), which leads to faster learning in Neural Networks."></div>
                <div class="flashcard" data-question="What is ReLU, and why is it commonly used?" data-answer="ReLU (Rectified Linear Unit) outputs x if x > 0, otherwise 0. It prevents the vanishing gradient problem."></div>
                <div class="flashcard" data-question="What is the difference between cost function and loss function?" data-answer="Loss function measures error for a single data point, while cost function is the average loss over all data points."></div>
                <div class="flashcard" data-question="What are common loss functions in regression and classification?" data-answer="MSE (L2 Loss) for regression, MAE (L1 Loss) for robust regression, and Cross-Entropy for classification."></div>
                <div class="flashcard" data-question="What is Early Stopping in Neural Networks?" data-answer="A technique that stops training when validation loss stops improving, preventing overfitting."></div>
                <div class="flashcard" data-question="What is Dropout in Neural Networks?" data-answer="Dropout randomly disables neurons during training to improve generalization and prevent overfitting."></div>
                <div class="flashcard" data-question="Why do Neural Networks use activation functions?" data-answer="To introduce non-linearity, enabling the model to learn complex relationships."></div>
            </div>
        </section>        
        
        <section id="notes">
            <h2>Revision Notes</h2>
            <article>
                <h3>üîπ Linear Regression</h3>
                <p>Linear Regression models the relationship between independent variables (X) and a dependent variable (Y) using a straight-line equation:</p>
                <pre><code>
        ≈∑ = w0 + w1x1 + w2x2 + ... + wn*xn
                </code></pre>
                <ul>
                    <li><strong>Intercept (w0):</strong> The starting point of the model.</li>
                    <li><strong>Weights (w1, w2,..., wn):</strong> The coefficients that define the influence of each feature.</li>
                    <li><strong>Mean Squared Error (MSE):</strong> The most common loss function for regression, calculated as:
                        <pre><code>MSE = (1/n) Œ£ (yi - ≈∑i)¬≤</code></pre>
                    </li>
                </ul>
        
                <h3>üîπ Logistic Regression</h3>
                <p>Unlike Linear Regression, Logistic Regression is used for classification tasks. It applies the **sigmoid function** to transform linear outputs into probabilities.</p>
                <pre><code>
        p = 1 / (1 + e^-(w0 + w1x1 + w2x2 + ... + wn*xn))
                </code></pre>
        
                <h3>üîπ Neural Networks (MLP)</h3>
                <p>Neural Networks extend regression by introducing hidden layers with activation functions:</p>
                <ul>
                    <li><strong>Input Layer:</strong> Receives input features.</li>
                    <li><strong>Hidden Layers:</strong> Applies transformations using weights and activation functions.</li>
                    <li><strong>Output Layer:</strong> Produces final predictions.</li>
                </ul>
        
                <h3>üîπ Activation Functions</h3>
                <ul>
                    <li><strong>Sigmoid:</strong> Compresses values between 0 and 1, useful for classification.</li>
                    <li><strong>TanH:</strong> Similar to Sigmoid but centered at 0 (-1 to 1), leading to faster training.</li>
                    <li><strong>ReLU:</strong> Outputs x if x > 0, otherwise 0, solving vanishing gradient issues.</li>
                </ul>
        
                <h3>üîπ Loss Functions</h3>
                <ul>
                    <li><strong>MSE:</strong> Used for regression, squares errors to emphasize larger deviations.</li>
                    <li><strong>MAE:</strong> More robust to outliers than MSE.</li>
                    <li><strong>Cross-Entropy:</strong> Used for classification, penalizes incorrect predictions heavily.</li>
                </ul>
        
                <h3>üîπ Regularization Techniques</h3>
                <ul>
                    <li><strong>Dropout:</strong> Disables random neurons during training to improve generalization.</li>
                    <li><strong>Early Stopping:</strong> Stops training when validation loss stops improving.</li>
                </ul>
            </article>
        </section>        
        
        <section id="key-code">
            <h2>Key Code / Functions</h2>
            
            <h3>1. Linear Regression in Python</h3>
            <p>Linear regression is a fundamental machine learning algorithm used for predicting a continuous target variable based on input features. It fits a straight line to the data by minimizing the sum of squared residuals (differences between actual and predicted values).</p>
            <pre><code>
        # Import necessary libraries
        import numpy as np
        import pandas as pd
        import matplotlib.pyplot as plt
        from sklearn.model_selection import train_test_split
        from sklearn.linear_model import LinearRegression
        from sklearn.metrics import mean_squared_error
        
        # Generate synthetic data
        np.random.seed(42)
        X = 2 * np.random.rand(100, 1)
        y = 4 + 3 * X + np.random.randn(100, 1)
        
        # Splitting data into training and testing sets
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # Train Linear Regression model
        model = LinearRegression()
        model.fit(X_train, y_train)
        
        # Make predictions
        y_pred = model.predict(X_test)
        
        # Evaluate performance
        mse = mean_squared_error(y_test, y_pred)
        print("Mean Squared Error:", mse)
        
        # Plot regression line
        plt.scatter(X, y, label="Data Points")
        plt.plot(X_test, y_pred, color="red", label="Regression Line")
        plt.legend()
        plt.show()
            </code></pre>
            <p><strong>Explanation:</strong></p>
            <ul>
                <li><strong>Generate Data:</strong> Creates a synthetic dataset where <code>y = 4 + 3X + noise</code>.</li>
                <li><strong>Splitting Data:</strong> Splits data into training (80%) and testing (20%) subsets.</li>
                <li><strong>Training Model:</strong> Fits a linear regression model to the training data.</li>
                <li><strong>Prediction:</strong> Uses the model to predict outcomes for the test set.</li>
                <li><strong>Evaluation:</strong> Computes the mean squared error (MSE) to measure model accuracy.</li>
                <li><strong>Visualization:</strong> Plots the regression line alongside the data points.</li>
            </ul>
        
            <h3>2. Logistic Regression for Classification</h3>
            <p>Logistic regression is a supervised learning algorithm used for binary classification problems. It predicts the probability that a given input belongs to a certain class using the sigmoid function.</p>
            <pre><code>
        from sklearn.linear_model import LogisticRegression
        from sklearn.metrics import accuracy_score
        
        # Generate synthetic classification dataset
        X = np.random.rand(100, 2)
        y = (X[:, 0] + X[:, 1] > 1).astype(int)  # Create a binary target variable
        
        # Split into training and test sets
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # Train a Logistic Regression model
        log_reg = LogisticRegression()
        log_reg.fit(X_train, y_train)
        
        # Make predictions
        y_pred = log_reg.predict(X_test)
        
        # Calculate accuracy
        accuracy = accuracy_score(y_test, y_pred)
        print("Model Accuracy:", accuracy)
            </code></pre>
            <p><strong>Explanation:</strong></p>
            <ul>
                <li><strong>Generating Data:</strong> Creates a dataset where the target class is based on a sum threshold.</li>
                <li><strong>Splitting Data:</strong> Divides the dataset into training and test sets.</li>
                <li><strong>Training the Model:</strong> Fits a logistic regression model to classify data points.</li>
                <li><strong>Prediction:</strong> Uses the model to classify test samples.</li>
                <li><strong>Evaluation:</strong> Computes accuracy to assess model performance.</li>
            </ul>
        
            <h3>3. Implementing a Neural Network with TensorFlow</h3>
            <p>Neural networks consist of multiple layers of neurons used to model complex patterns in data. We implement a simple feedforward neural network using TensorFlow and Keras.</p>
            <pre><code>
        import tensorflow as tf
        from tensorflow.keras.models import Sequential
        from tensorflow.keras.layers import Dense
        from tensorflow.keras.optimizers import Adam
        
        # Generate synthetic dataset
        X_train = np.random.rand(1000, 5)
        y_train = (X_train[:, 0] + X_train[:, 1] > 1).astype(int)
        
        # Define the Neural Network model
        model = Sequential([
            Dense(10, activation='relu', input_shape=(5,)),
            Dense(5, activation='relu'),
            Dense(1, activation='sigmoid')  # Sigmoid for binary classification
        ])
        
        # Compile model
        model.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy'])
        
        # Train the model
        model.fit(X_train, y_train, epochs=20, batch_size=10, verbose=1)
            </code></pre>
            <p><strong>Explanation:</strong></p>
            <ul>
                <li><strong>Data Creation:</strong> Generates a dataset with 5 input features.</li>
                <li><strong>Defining the Model:</strong> Uses three layers with ReLU and Sigmoid activations.</li>
                <li><strong>Compiling the Model:</strong> Uses Adam optimizer and binary cross-entropy loss.</li>
                <li><strong>Training:</strong> Fits the model for 20 epochs with batch size 10.</li>
            </ul>
        </section>
        
        
        
        
    </main>
    
    <footer>
        <p>&copy; 2025 Thomas Baker - MSc Data Science & AI Revision</p>
    </footer>
    
    <script defer src="aml-flashcards.js"></script>
</body>
</html>
