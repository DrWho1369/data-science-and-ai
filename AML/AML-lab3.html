<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AML Lab 3 - Feature Selection & Resampling</title>
    <link rel="stylesheet" href="aml-styles.css">
</head>
<body>
    <header class="hero">
        <h1>AML Lab 3 - Feature Selection & Resampling</h1>
    </header>
    
    <main>
        <a href="AML-homepage.html" class="back-button">‚Üê Back to Homepage</a>
        <section id="flashcards">
            <h2>Flashcards</h2>
            <div class="flashcard-container">
                <div class="flashcard" data-question="What is feature selection in machine learning?" data-answer="Feature selection is the process of selecting the most relevant features in a dataset to improve model performance and reduce overfitting."></div>
                <div class="flashcard" data-question="What are three benefits of feature selection?" data-answer="It reduces overfitting, improves accuracy, and decreases training time."></div>
                <div class="flashcard" data-question="What is SelectKBest in Scikit-Learn?" data-answer="SelectKBest is a method that selects the top k features based on statistical tests, such as chi-squared for classification tasks."></div>
                <div class="flashcard" data-question="What is Recursive Feature Elimination (RFE)?" data-answer="RFE recursively removes features and builds models to determine the best subset that contributes most to prediction."></div>
                <div class="flashcard" data-question="How does Principal Component Analysis (PCA) reduce dimensionality?" data-answer="PCA transforms data into a lower-dimensional space by selecting principal components that capture the most variance."></div>
                <div class="flashcard" data-question="What is the purpose of feature importance in decision trees?" data-answer="Feature importance ranks input features based on their contribution to model predictions, helping to understand feature relevance."></div>
                <div class="flashcard" data-question="Why is resampling important in model evaluation?" data-answer="Resampling methods provide more reliable performance estimates by using different data splits for training and validation."></div>
                <div class="flashcard" data-question="What is k-fold cross-validation?" data-answer="A method that splits the dataset into k subsets (folds), training the model on k-1 folds and validating it on the remaining fold."></div>
                <div class="flashcard" data-question="What is Leave-One-Out Cross-Validation (LOOCV)?" data-answer="LOOCV trains on all but one data point and tests on the remaining one, repeating this process for all data points."></div>
                <div class="flashcard" data-question="How does Shuffle Split differ from k-fold cross-validation?" data-answer="Shuffle Split randomly splits the data multiple times, instead of using contiguous folds, reducing variance in performance estimates."></div>
            </div>
        </section>
        
        <section id="lab3">
            <h2>üõ†Ô∏è Lab 3: Feature Selection & Resampling</h2>
            <p>Feature selection is crucial for improving <strong>model accuracy, reducing overfitting, and minimizing training time</strong>. This lab covers multiple feature selection methods and resampling techniques using the <strong>Pima Indians Diabetes Dataset</strong>.</p>
        
            <h3>üìå Univariate Feature Selection</h3>
            <p>Uses <strong>statistical tests</strong> to select the most important features. The Chi-squared test is used to evaluate which features are most relevant for predicting diabetes.</p>
            <pre><code>
from pandas import read_csv
from numpy import set_printoptions
from sklearn.feature_selection import SelectKBest, chi2

# Load dataset
filename = 'pima-indians-diabetes.data.csv'
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(filename, names=names)

# Split features (X) and target variable (Y)
X = data.iloc[:, :-1]
Y = data.iloc[:, -1]

# Apply Chi-squared feature selection
test = SelectKBest(score_func=chi2, k=4)
fit = test.fit(X, Y)

# Display scores
set_printoptions(precision=3)
print(fit.scores_)
features = fit.transform(X)

# Show the selected features
print(features[:5, :])
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li><strong>SelectKBest(score_func=chi2, k=4)</strong> selects the 4 most relevant features.</li>
                <li>The <code>chi2</code> test measures the dependency between features and the target variable, assuming non-negative numerical values.</li>
                <li>Higher scores indicate stronger relevance of a feature to the target variable.</li>
            </ul>
        
            <h3>üìå Recursive Feature Elimination (RFE)</h3>
            <p>RFE works by removing features <strong>recursively</strong>, evaluating model performance at each step.</p>
            <pre><code>
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

# Define logistic regression model
model = LogisticRegression(solver='liblinear')

# Apply RFE to select top 3 features
rfe = RFE(model, n_features_to_select=3)
fit = rfe.fit(X, Y)

print("Num Features:", fit.n_features_)
print("Selected Features:", fit.support_)
print("Feature Ranking:", fit.ranking_)
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li><strong>RFE iteratively removes the least important features</strong> and re-evaluates model performance.</li>
                <li>It identifies the optimal subset of features to reduce noise and enhance prediction accuracy.</li>
                <li>Features ranked <strong>1</strong> are the most important features selected by the model.</li>
            </ul>
        
            <h3>üìå Principal Component Analysis (PCA)</h3>
            <p>PCA is a <strong>dimensionality reduction technique</strong> that transforms the dataset into a smaller number of components.</p>
            <pre><code>
from sklearn.decomposition import PCA

# Apply PCA to reduce to 3 principal components
pca = PCA(n_components=3)
fit = pca.fit(X)

# Display variance explained by each component
print("Explained Variance:", fit.explained_variance_ratio_)
print(fit.components_)
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>PCA finds new <strong>uncorrelated components</strong> that capture the most variance in the data.</li>
                <li>It helps when dealing with <strong>highly correlated features</strong> by reducing redundancy.</li>
                <li>The <strong>explained variance ratio</strong> shows how much of the data's information is retained.</li>
            </ul>
        
            <h3>üìå Feature Importance using Random Forest</h3>
            <p>Tree-based models like <strong>Random Forest</strong> can automatically estimate feature importance.</p>
            <pre><code>
from sklearn.ensemble import ExtraTreesClassifier

# Train Extra Trees Classifier
model = ExtraTreesClassifier()
model.fit(X, Y)

# Display feature importance scores
print(model.feature_importances_)
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>Random Forest assigns <strong>importance scores</strong> based on how much each feature improves decision-making.</li>
                <li>Higher values indicate <strong>more informative features</strong> for the prediction task.</li>
            </ul>
        
            <h3>üìå Resampling Techniques</h3>
            <p>Resampling ensures that model performance is evaluated fairly across different data subsets.</p>
            <ul>
                <li><strong>Train-Test Split</strong>: Basic method where data is divided into training and testing sets.</li>
                <li><strong>K-Fold Cross Validation</strong>: More reliable approach where data is split into multiple subsets to test model performance across different folds.</li>
                <li><strong>Leave-One-Out Cross Validation (LOOCV)</strong>: Extreme case of K-Fold CV, where one data point is used for testing at a time.</li>
            </ul>
        
            <h3>üìå Summary</h3>
            <ul>
                <li>We explored <strong>feature selection methods</strong>: Univariate selection, RFE, PCA, and Random Forest importance.</li>
                <li>Implemented <strong>resampling techniques</strong> to improve model evaluation.</li>
                <li>Cross-validation methods help ensure <strong>reliable performance estimates</strong>.</li>
            </ul>
        </section>
    </main>
    
    <footer>
        <p>&copy; 2025 Thomas Baker - MSc Data Science & AI Revision</p>
    </footer>
    
    <script defer src="aml-flashcards.js"></script>
</body>
</html>
