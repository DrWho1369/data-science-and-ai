<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AML Lab 3 - Feature Selection & Resampling</title>
    <link rel="stylesheet" href="aml-styles.css">
</head>
<body>
    <header class="hero">
        <h1>AML Lab 3 - Feature Selection & Resampling</h1>
    </header>
    
    <main>
        <a href="AML-homepage.html" class="back-button">‚Üê Back to Homepage</a>

        <section id="lab3">
            <h2>üõ†Ô∏è Lab 3: Feature Selection & Resampling</h2>
            <p>Feature selection is crucial for improving <strong>model accuracy, reducing overfitting, and minimizing training time</strong>. This lab covers multiple feature selection methods and resampling techniques using the <strong>Pima Indians Diabetes Dataset</strong>.</p>
        
            <h3>üìå Univariate Feature Selection</h3>
            <p>Uses <strong>statistical tests</strong> to select the most important features. The Chi-squared test is used to evaluate which features are most relevant for predicting diabetes.</p>
            <pre><code>
from pandas import read_csv
from numpy import set_printoptions
from sklearn.feature_selection import SelectKBest, chi2

# Load dataset
filename = 'pima-indians-diabetes.data.csv'
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(filename, names=names)

# Split features (X) and target variable (Y)
X = data.iloc[:, :-1]
Y = data.iloc[:, -1]

# Apply Chi-squared feature selection
test = SelectKBest(score_func=chi2, k=4)
fit = test.fit(X, Y)

# Display scores
set_printoptions(precision=3)
print(fit.scores_)
features = fit.transform(X)

# Show the selected features
print(features[:5, :])
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li><strong>SelectKBest(score_func=chi2, k=4)</strong> selects the 4 most relevant features.</li>
                <li>The <code>chi2</code> test measures the dependency between features and the target variable, assuming non-negative numerical values.</li>
                <li>Higher scores indicate stronger relevance of a feature to the target variable.</li>
            </ul>
        
            <h3>üìå Recursive Feature Elimination (RFE)</h3>
            <p>RFE works by removing features <strong>recursively</strong>, evaluating model performance at each step.</p>
            <pre><code>
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

# Define logistic regression model
model = LogisticRegression(solver='liblinear')

# Apply RFE to select top 3 features
rfe = RFE(model, n_features_to_select=3)
fit = rfe.fit(X, Y)

print("Num Features:", fit.n_features_)
print("Selected Features:", fit.support_)
print("Feature Ranking:", fit.ranking_)
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li><strong>RFE iteratively removes the least important features</strong> and re-evaluates model performance.</li>
                <li>It identifies the optimal subset of features to reduce noise and enhance prediction accuracy.</li>
                <li>Features ranked <strong>1</strong> are the most important features selected by the model.</li>
            </ul>
        
            <h3>üìå Principal Component Analysis (PCA)</h3>
            <p>PCA is a <strong>dimensionality reduction technique</strong> that transforms the dataset into a smaller number of components.</p>
            <pre><code>
from sklearn.decomposition import PCA

# Apply PCA to reduce to 3 principal components
pca = PCA(n_components=3)
fit = pca.fit(X)

# Display variance explained by each component
print("Explained Variance:", fit.explained_variance_ratio_)
print(fit.components_)
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>PCA finds new <strong>uncorrelated components</strong> that capture the most variance in the data.</li>
                <li>It helps when dealing with <strong>highly correlated features</strong> by reducing redundancy.</li>
                <li>The <strong>explained variance ratio</strong> shows how much of the data's information is retained.</li>
            </ul>
        
            <h3>üìå Feature Importance using Random Forest</h3>
            <p>Tree-based models like <strong>Random Forest</strong> can automatically estimate feature importance.</p>
            <pre><code>
from sklearn.ensemble import ExtraTreesClassifier

# Train Extra Trees Classifier
model = ExtraTreesClassifier()
model.fit(X, Y)

# Display feature importance scores
print(model.feature_importances_)
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>Random Forest assigns <strong>importance scores</strong> based on how much each feature improves decision-making.</li>
                <li>Higher values indicate <strong>more informative features</strong> for the prediction task.</li>
            </ul>
        
            <h3>üìå Resampling Techniques</h3>
            <p>Resampling ensures that model performance is evaluated fairly across different data subsets.</p>
            <ul>
                <li><strong>Train-Test Split</strong>: Basic method where data is divided into training and testing sets.</li>
                <li><strong>K-Fold Cross Validation</strong>: More reliable approach where data is split into multiple subsets to test model performance across different folds.</li>
                <li><strong>Leave-One-Out Cross Validation (LOOCV)</strong>: Extreme case of K-Fold CV, where one data point is used for testing at a time.</li>
            </ul>
        
            <h3>üìå Summary</h3>
            <ul>
                <li>We explored <strong>feature selection methods</strong>: Univariate selection, RFE, PCA, and Random Forest importance.</li>
                <li>Implemented <strong>resampling techniques</strong> to improve model evaluation.</li>
                <li>Cross-validation methods help ensure <strong>reliable performance estimates</strong>.</li>
            </ul>
        </section>
    </main>
    
    <footer>
        <p>&copy; 2025 Thomas Baker - MSc Data Science & AI Revision</p>
    </footer>
    
    <script defer src="aml-flashcards.js"></script>
</body>
</html>
