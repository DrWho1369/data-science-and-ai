<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AML Lab 3 - Feature Selection & Resampling</title>
    <link rel="stylesheet" href="aml-styles.css">
</head>
<body>
    <header>
        <h1>AML Lab 3 - Feature Selection & Resampling</h1>
    </header>
    
    <main>
        <a href="AML-homepage.html" class="back-button">‚Üê Back to Homepage</a>

        <section id="lab3">
            <h2>üõ†Ô∏è Lab 3: Feature Selection & Resampling</h2>
            <p>Feature selection is crucial for improving <strong>model accuracy, reducing overfitting, and minimizing training time</strong>. This lab covers multiple feature selection methods and resampling techniques using the <strong>Pima Indians Diabetes Dataset</strong>.</p>
        
            <h3>üìå Univariate Feature Selection</h3>
        
            <p>Uses <strong>statistical tests</strong> to select the most important features. The Chi-squared test is used to evaluate which features are most relevant for predicting diabetes.</p>
            <pre><code>
        from pandas import read_csv
        from numpy import set_printoptions
        from sklearn.feature_selection import SelectKBest, chi2
        
        # Load dataset
        filename = 'pima-indians-diabetes.data.csv'
        names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
        data = read_csv(filename, names=names)
        
        # Split features (X) and target variable (Y)
        X = data.iloc[:, :-1]
        Y = data.iloc[:, -1]
        
        # Apply Chi-squared feature selection
        test = SelectKBest(score_func=chi2, k=4)
        fit = test.fit(X, Y)
        
        # Display scores
        set_printoptions(precision=3)
        print(fit.scores_)
        features = fit.transform(X)
        
        # Show the selected features
        print(features[:5, :])
            </code></pre>
            <p><strong>Explanation:</strong> 
                <ul>
                    <li><strong>SelectKBest(score_func=chi2, k=4)</strong> selects the 4 most relevant features.</li>
                    <li>The <code>chi2</code> test is used since the dataset contains <strong>non-negative numerical values</strong>.</li>
                    <li>Features with the <strong>highest scores</strong> are chosen as the most important predictors.</li>
                </ul>
            </p>
        
            <h3>üìå Recursive Feature Elimination (RFE)</h3>
            <p>RFE works by removing features <strong>recursively</strong>, evaluating model performance at each step.</p>
            <pre><code>
        from sklearn.feature_selection import RFE
        from sklearn.linear_model import LogisticRegression
        
        # Define logistic regression model
        model = LogisticRegression(solver='liblinear')
        
        # Apply RFE to select top 3 features
        rfe = RFE(model, n_features_to_select=3)
        fit = rfe.fit(X, Y)
        
        print("Num Features:", fit.n_features_)
        print("Selected Features:", fit.support_)
        print("Feature Ranking:", fit.ranking_)
            </code></pre>
            <p><strong>Explanation:</strong> 
                <ul>
                    <li>Uses <strong>Logistic Regression</strong> to determine feature importance.</li>
                    <li>The top 3 selected features are marked <strong>True</strong> in <code>fit.support_</code>.</li>
                    <li>Ranking shows how important each feature is, with <strong>1 being most important</strong>.</li>
                </ul>
            </p>
        
            <h3>üìå Principal Component Analysis (PCA)</h3>
            <p>PCA is a <strong>dimensionality reduction technique</strong> that transforms the dataset into a smaller number of components.</p>
            <pre><code>
        from sklearn.decomposition import PCA
        
        # Apply PCA to reduce to 3 principal components
        pca = PCA(n_components=3)
        fit = pca.fit(X)
        
        # Display variance explained by each component
        print("Explained Variance:", fit.explained_variance_ratio_)
        print(fit.components_)
            </code></pre>
            <p><strong>Explanation:</strong>
                <ul>
                    <li>PCA transforms original features into <strong>new uncorrelated features</strong>.</li>
                    <li>It selects <strong>3 principal components</strong>, preserving <strong>maximum variance</strong>.</li>
                    <li>The variance ratio tells us how much <strong>information each component retains</strong>.</li>
                </ul>
            </p>
        
            <h3>üìå Feature Importance using Random Forest</h3>
            <p>Tree-based models like <strong>Random Forest</strong> can automatically estimate feature importance.</p>
            <pre><code>
        from sklearn.ensemble import ExtraTreesClassifier
        
        # Train Extra Trees Classifier
        model = ExtraTreesClassifier()
        model.fit(X, Y)
        
        # Display feature importance scores
        print(model.feature_importances_)
            </code></pre>
            <p><strong>Explanation:</strong> 
                <ul>
                    <li><strong>ExtraTreesClassifier</strong> is an <strong>ensemble learning method</strong> that builds multiple trees.</li>
                    <li>Higher feature importance scores indicate <strong>stronger predictive power</strong>.</li>
                </ul>
            </p>
        
            <h3>üìå Train-Test Split</h3>
            <p>The most basic resampling method for evaluating models.</p>
            <pre><code>
        from sklearn.model_selection import train_test_split
        from sklearn.linear_model import LogisticRegression
        
        # Split data into 67% training, 33% testing
        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=7)
        
        # Train logistic regression model
        model = LogisticRegression(solver='liblinear')
        model.fit(X_train, Y_train)
        
        # Evaluate model accuracy
        accuracy = model.score(X_test, Y_test)
        print(f"Accuracy: {accuracy:.3f}")
            </code></pre>
            <p><strong>Explanation:</strong>
                <ul>
                    <li>Splits data into <strong>training (67%) and testing (33%)</strong> subsets.</li>
                    <li>The model is trained using <strong>Logistic Regression</strong> and evaluated on test data.</li>
                    <li>Accuracy score represents <strong>model performance</strong> on unseen data.</li>
                </ul>
            </p>
        
            <h3>üìå K-Fold Cross Validation</h3>
            <p>Cross-validation improves model evaluation by testing on <strong>multiple train-test splits</strong>.</p>
            <pre><code>
        from sklearn.model_selection import KFold, cross_val_score
        
        # Perform 10-fold cross-validation
        kfold = KFold(n_splits=10, random_state=7, shuffle=True)
        results = cross_val_score(model, X, Y, cv=kfold)
        
        print(f"Accuracy: {results.mean()*100:.3f}% ¬± {results.std()*100:.3f}%")
            </code></pre>
            <p><strong>Explanation:</strong>
                <ul>
                    <li>Data is split into <strong>10 equal folds</strong>.</li>
                    <li>Each fold is used as a test set <strong>once</strong>, and the remaining folds are used for training.</li>
                    <li>Reduces <strong>variance</strong> in model performance evaluation.</li>
                </ul>
            </p>
        
            <h3>üìå Leave-One-Out Cross Validation (LOOCV)</h3>
            <p>LOOCV is an extreme case of <strong>K-Fold CV</strong>, where <strong>only one data point is left for testing each time</strong>.</p>
            <pre><code>
        from sklearn.model_selection import LeaveOneOut
        
        # Perform LOOCV
        loocv = LeaveOneOut()
        results = cross_val_score(model, X, Y, cv=loocv)
        
        print(f"Accuracy: {results.mean()*100:.3f}% ¬± {results.std()*100:.3f}%")
            </code></pre>
            <p><strong>Explanation:</strong>
                <ul>
                    <li>Trains the model on <strong>all but one</strong> data point.</li>
                    <li>Repeats for every data point in the dataset.</li>
                    <li>More computationally expensive but provides <strong>low bias</strong> in evaluation.</li>
                </ul>
            </p>
        
            <h3>üìå Summary</h3>
            <ul>
                <li><strong>Feature selection methods</strong>: Chi-Squared, RFE, PCA, and Random Forest importance.</li>
                <li><strong>Resampling techniques</strong>: Train-Test Split, K-Fold Cross Validation, LOOCV.</li>
                <li>Cross-validation methods help <strong>improve model performance evaluation</strong>.</li>
            </ul>
        
        </section>        
        
        
        
    </main>
    
    <footer>
        <p>&copy; 2025 Thomas Baker - MSc Data Science & AI Revision</p>
    </footer>
    
    <script defer src="aml-flashcards.js"></script>
</body>
</html>
