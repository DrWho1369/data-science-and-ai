<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AML Lab 5 - Automating and Saving Models</title>
    <link rel="stylesheet" href="aml-styles.css">
</head>
<body>
    <header>
        <h1>AML Lab 5 - Automating and Saving Models</h1>
    </header>
    
    <main>
        <a href="AML-homepage.html" class="back-button">‚Üê Back to Homepage</a>

        <section id="lab5">
            <h2>‚öôÔ∏è Lab 5: Automating and Saving Models</h2>
            <p>Building a machine learning model requires multiple steps, including **data preprocessing, feature selection, training, evaluation, and model deployment**. Instead of manually repeating these steps, we can **automate the entire pipeline** and ensure **reproducibility and efficiency**. In this lab, we will:</p>
            <ul>
                <li>Use **Pipelines** to automate preprocessing and model training.</li>
                <li>Prevent **data leakage** by ensuring transformations happen only on training data.</li>
                <li>Use **FeatureUnion** to combine multiple feature selection techniques.</li>
                <li>Save and load models for **future use without retraining**.</li>
            </ul>
        
            <h3>üìå Automating Workflows with Pipelines</h3>
            <p>A **Pipeline** chains multiple processing steps into a single workflow, ensuring **data transformations occur only on training data** and preventing data leakage.</p>
            <pre><code>
from pandas import read_csv
from sklearn.model_selection import KFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# Load dataset
filename = 'pima-indians-diabetes.data.csv'
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(filename, names=names)

X = data.iloc[:, :-1]
Y = data.iloc[:, -1]

# Define pipeline steps
steps = [('standardize', StandardScaler()), ('lda', LinearDiscriminantAnalysis())]
model = Pipeline(steps)

# Evaluate pipeline with cross-validation
kfold = KFold(n_splits=10, random_state=7, shuffle=True)
results = cross_val_score(model, X, Y, cv=kfold)

print(f"Mean Accuracy: {results.mean():.3f}")
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li><code>StandardScaler()</code> ensures **all features are on the same scale**, improving model performance.</li>
                <li><code>Pipeline(steps)</code> **chains transformations and modeling into a single object**, reducing repetitive code.</li>
                <li><code>cross_val_score()</code> applies **10-fold cross-validation**, reducing bias by evaluating on different subsets of data.</li>
            </ul>
        
            <h3>üìå Extracting Model Coefficients</h3>
            <p>Understanding **how a model makes decisions** is important for interpretability. After training, we can extract **Linear Discriminant Analysis (LDA) coefficients**.</p>
            <pre><code>
model.fit(X, Y)
lda_model = model.named_steps['lda']

print("LDA Coefficients:")
print(lda_model.coef_)
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>The model is first trained using <code>model.fit(X, Y)</code>.</li>
                <li>We retrieve the trained **LDA model** using <code>model.named_steps['lda']</code>.</li>
                <li>The **LDA coefficients** show how much weight each feature has in determining the classification.</li>
            </ul>
        
            <h3>üìå Feature Engineering with FeatureUnion</h3>
            <p>Instead of selecting one feature engineering technique, we can **combine multiple methods** using <strong>FeatureUnion</strong>.</p>
            <pre><code>
from sklearn.pipeline import FeatureUnion
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest

# Define feature extraction methods
features = [('pca', PCA(n_components=3)), ('select_best', SelectKBest(k=6))]
feature_union = FeatureUnion(features)

# Create pipeline
pipeline_steps = [('feature_union', feature_union), ('logistic', LogisticRegression())]
model = Pipeline(pipeline_steps)

# Evaluate
results = cross_val_score(model, X, Y, cv=kfold)
print(f"Mean Accuracy: {results.mean():.3f}")
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li><code>PCA(n_components=3)</code> reduces dimensionality while retaining the most **variance in the data**.</li>
                <li><code>SelectKBest(k=6)</code> selects the **top 6 features based on statistical significance**.</li>
                <li><code>FeatureUnion</code> **combines both methods** to create a **richer feature set**.</li>
            </ul>
        
            <h3>üìå Saving and Loading Models</h3>
            <p>Once a model is trained, we **don‚Äôt need to retrain it every time**‚Äîwe can save it using **Pickle or Joblib**.</p>
            <h4>Saving a Model with Pickle</h4>
            <pre><code>
import pickle

# Save model
with open('final_model.pkl', 'wb') as f:
    pickle.dump(model, f)

# Load model
with open('final_model.pkl', 'rb') as f:
    loaded_model = pickle.load(f)

print(loaded_model.predict(X[:5]))  # Predict using loaded model
            </code></pre>
            <h4>Saving a Model with Joblib</h4>
            <pre><code>
from joblib import dump, load

# Save
dump(model, 'final_model.joblib')

# Load
loaded_model = load('final_model.joblib')
print(loaded_model.predict(X[:5]))
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li><code>pickle</code> is used for **serializing Python objects**, including models.</li>
                <li><code>joblib</code> is optimized for **handling large NumPy arrays**, making it faster for machine learning models.</li>
            </ul>
        
            <h3>üìå Summary</h3>
            <ul>
                <li>We automated preprocessing and modeling using **Pipelines**.</li>
                <li>We extracted **LDA model coefficients** to interpret feature importance.</li>
                <li>We used **FeatureUnion** to combine multiple feature engineering techniques.</li>
                <li>We saved and loaded trained models using **Pickle and Joblib**.</li>
            </ul>
        </section>
    </main>
    
    <footer>
        <p>&copy; 2025 Thomas Baker - MSc Data Science & AI Revision</p>
    </footer>
    
    <script defer src="aml-flashcards.js"></script>
</body>
</html>
