<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AML Lab 5 - Automating and Saving Models</title>
    <link rel="stylesheet" href="aml-styles.css">
</head>
<body>
    <header class="hero">
        <h1>AML Lab 5 - Automating and Saving Models</h1>
    </header>
    
    <main>
        <a href="AML-homepage.html" class="back-button">‚Üê Back to Homepage</a>
        <section id="flashcards">
            <h2>Flashcards</h2>
            <div class="flashcard-container">
                <div class="flashcard" data-question="What is a machine learning pipeline?" data-answer="A sequence of data processing steps chained together, ensuring proper workflow from preprocessing to model evaluation."></div>
                <div class="flashcard" data-question="Why are pipelines useful in machine learning?" data-answer="They prevent data leakage, automate workflows, and ensure consistency in data processing and model evaluation."></div>
                <div class="flashcard" data-question="How does Scikit-Learn implement pipelines?" data-answer="Using the Pipeline class, where each step (e.g., scaling, feature selection, model training) is executed sequentially."></div>
                <div class="flashcard" data-question="What problem does data leakage cause?" data-answer="Data leakage gives the model access to information from the test set during training, leading to overly optimistic performance estimates."></div>
                <div class="flashcard" data-question="What is cross-validation in machine learning?" data-answer="A technique for evaluating a model by splitting the dataset into training and validation sets multiple times to get a reliable performance estimate."></div>
                <div class="flashcard" data-question="Why is cross-validation used within pipelines?" data-answer="To ensure that data preprocessing, feature selection, and model evaluation occur only within each fold, preventing data leakage."></div>
                <div class="flashcard" data-question="What does the FeatureUnion class in Scikit-Learn do?" data-answer="It combines multiple feature extraction methods into a single dataset, allowing diverse feature transformations in a pipeline."></div>
                <div class="flashcard" data-question="How does feature extraction affect a machine learning model?" data-answer="It transforms raw data into a more useful format, improving performance and reducing complexity."></div>
                <div class="flashcard" data-question="What are ensemble methods in machine learning?" data-answer="Techniques that combine multiple models to improve accuracy and robustness, such as Bagging, Boosting, and Voting."></div>
                <div class="flashcard" data-question="How does bagging improve model performance?" data-answer="Bagging reduces variance by training multiple models on different subsets of data and averaging their predictions."></div>
                <div class="flashcard" data-question="What is the purpose of boosting algorithms like AdaBoost?" data-answer="Boosting trains models sequentially, where each model corrects the mistakes of the previous one, improving performance."></div>
                <div class="flashcard" data-question="What is a voting classifier?" data-answer="A model that combines predictions from multiple algorithms (e.g., logistic regression, decision trees) to improve overall performance."></div>
                <div class="flashcard" data-question="What is hyperparameter tuning?" data-answer="The process of optimizing model parameters to improve performance, typically using GridSearchCV or RandomizedSearchCV."></div>
                <div class="flashcard" data-question="What is the difference between Grid Search and Random Search?" data-answer="Grid Search evaluates all parameter combinations systematically, while Random Search samples parameters randomly for faster tuning."></div>
                <div class="flashcard" data-question="What is the role of Pickle in machine learning?" data-answer="Pickle serializes and saves trained models for later use, allowing them to be reloaded without retraining."></div>
                <div class="flashcard" data-question="How does Joblib differ from Pickle for saving models?" data-answer="Joblib is optimized for large NumPy arrays and is more efficient when working with complex machine learning models."></div>
                <div class="flashcard" data-question="Why is saving machine learning models important?" data-answer="It allows trained models to be reused, deployed, and evaluated without needing to retrain from scratch."></div>
                <div class="flashcard" data-question="What are the risks of model serialization?" data-answer="Compatibility issues may arise due to different Python or library versions when loading a saved model on another system."></div>
                <div class="flashcard" data-question="What is the main goal of automating machine learning workflows?" data-answer="To create a repeatable and scalable process that improves efficiency, reduces errors, and ensures consistency in model training."></div>
            </div>
        </section>
        
        <section id="lab5">
            <h2>‚öôÔ∏è Lab 5: Automating and Saving Models</h2>
            <p>Building a machine learning model requires multiple steps, including <strong>data preprocessing, feature selection, training, evaluation, and model deployment</strong>. Instead of manually repeating these steps, we can <strong>automate the entire pipeline</strong> and ensure <strong>reproducibility and efficiency</strong>. In this lab, we will:</p>
            <ul>
                <li>Use <strong>Pipelines</strong> to automate preprocessing and model training.</li>
                <li>Prevent <strong>data leakage</strong> by ensuring transformations happen only on training data.</li>
                <li>Use <strong>FeatureUnion</strong> to combine multiple feature selection techniques.</li>
                <li>Save and load models for <strong>future use without retraining</strong>.</li>
            </ul>
        
            <h3>üìå Automating Workflows with Pipelines</h3>
            <p>A <strong>Pipeline</strong> chains multiple processing steps into a single workflow, ensuring <strong>data transformations occur only on training data</strong> and preventing data leakage.</p>
            <pre><code>
from pandas import read_csv
from sklearn.model_selection import KFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# Load dataset
filename = 'pima-indians-diabetes.data.csv'
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(filename, names=names)

X = data.iloc[:, :-1]
Y = data.iloc[:, -1]

# Define pipeline steps
steps = [('standardize', StandardScaler()), ('lda', LinearDiscriminantAnalysis())]
model = Pipeline(steps)

# Evaluate pipeline with cross-validation
kfold = KFold(n_splits=10, random_state=7, shuffle=True)
results = cross_val_score(model, X, Y, cv=kfold)

print(f"Mean Accuracy: {results.mean():.3f}")
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li><code>StandardScaler()</code> ensures <strong>all features are on the same scale</strong>, improving model performance.</li>
                <li><code>Pipeline(steps)</code> <strong>chains transformations and modeling into a single object</strong>, reducing repetitive code.</li>
                <li><code>cross_val_score()</code> applies <strong>10-fold cross-validation</strong>, reducing bias by evaluating on different subsets of data.</li>
            </ul>
        
            <h3>üìå Extracting Model Coefficients</h3>
            <p>Understanding <strong>how a model makes decisions</strong> is important for interpretability. After training, we can extract <strong>Linear Discriminant Analysis (LDA) coefficients</strong>.</p>
            <pre><code>
model.fit(X, Y)
lda_model = model.named_steps['lda']

print("LDA Coefficients:")
print(lda_model.coef_)
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>The model is first trained using <code>model.fit(X, Y)</code>.</li>
                <li>We retrieve the trained <strong>LDA model</strong> using <code>model.named_steps['lda']</code>.</li>
                <li>The <strong>LDA coefficients</strong> show how much weight each feature has in determining the classification.</li>
            </ul>
        
            <h3>üìå Feature Engineering with FeatureUnion</h3>
            <p>Instead of selecting one feature engineering technique, we can <strong>combine multiple methods</strong> using <strong>FeatureUnion</strong>.</p>
            <pre><code>
from sklearn.pipeline import FeatureUnion
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest

# Define feature extraction methods
features = [('pca', PCA(n_components=3)), ('select_best', SelectKBest(k=6))]
feature_union = FeatureUnion(features)

# Create pipeline
pipeline_steps = [('feature_union', feature_union), ('logistic', LogisticRegression())]
model = Pipeline(pipeline_steps)

# Evaluate
results = cross_val_score(model, X, Y, cv=kfold)
print(f"Mean Accuracy: {results.mean():.3f}")
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li><code>PCA(n_components=3)</code> reduces dimensionality while retaining the most <strong>variance in the data</strong>.</li>
                <li><code>SelectKBest(k=6)</code> selects the <strong>top 6 features based on statistical significance</strong>.</li>
                <li><code>FeatureUnion</code> <strong>combines both methods</strong> to create a <strong>richer feature set</strong>.</li>
            </ul>
        
            <h3>üìå Saving and Loading Models</h3>
            <p>Once a model is trained, we <strong>don‚Äôt need to retrain it every time</strong>‚Äîwe can save it using <strong>Pickle or Joblib</strong>.</p>
            <h4>Saving a Model with Pickle</h4>
            <pre><code>
import pickle

# Save model
with open('final_model.pkl', 'wb') as f:
    pickle.dump(model, f)

# Load model
with open('final_model.pkl', 'rb') as f:
    loaded_model = pickle.load(f)

print(loaded_model.predict(X[:5]))  # Predict using loaded model
            </code></pre>
            <h4>Saving a Model with Joblib</h4>
            <pre><code>
from joblib import dump, load

# Save
dump(model, 'final_model.joblib')

# Load
loaded_model = load('final_model.joblib')
print(loaded_model.predict(X[:5]))
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li><code>pickle</code> is used for <strong>serializing Python objects</strong>, including models.</li>
                <li><code>joblib</code> is optimized for <strong>handling large NumPy arrays</strong>, making it faster for machine learning models.</li>
            </ul>
        
            <h3>üìå Summary</h3>
            <ul>
                <li>We automated preprocessing and modeling using <strong>Pipelines</strong>.</li>
                <li>We extracted <strong>LDA model coefficients</strong> to interpret feature importance.</li>
                <li>We used <strong>FeatureUnion</strong> to combine multiple feature engineering techniques.</li>
                <li>We saved and loaded trained models using <strong>Pickle and Joblib</strong>.</li>
            </ul>
        </section>
    </main>
    
    <footer>
        <p>&copy; 2025 Thomas Baker - MSc Data Science & AI Revision</p>
    </footer>
    
    <script defer src="aml-flashcards.js"></script>
</body>
</html>
