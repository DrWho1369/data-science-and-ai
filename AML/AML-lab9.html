<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AML Lab 9 - Long Short-Term Memory (LSTM) for Sequence Prediction & Classification</title>
    <link rel="stylesheet" href="aml-styles.css">
</head>
<body>
    <header class="hero">
        <h1>AML Lab 9 - Long Short-Term Memory (LSTM) for Sequence Prediction & Classification</h1>
    </header>
    
    <main>
        <a href="AML-homepage.html" class="back-button">‚Üê Back to Homepage</a>
        <section id="flashcards">
            <h2>Flashcards</h2>
            <div class="flashcard-container">
                <div class="flashcard" data-question="What is an LSTM?" data-answer="A Long Short-Term Memory (LSTM) network is a type of recurrent neural network (RNN) that can learn long-term dependencies in sequence data."></div>
                <div class="flashcard" data-question="Why are LSTMs preferred over standard RNNs?" data-answer="LSTMs use special 'gates' to control the flow of information, solving the vanishing gradient problem in long sequences."></div>
                <div class="flashcard" data-question="What is the purpose of the forget gate in an LSTM?" data-answer="It determines which information from the previous step should be discarded or kept."></div>
                <div class="flashcard" data-question="What dataset is used for time series prediction in this lab?" data-answer="The international airline passengers dataset, which contains monthly passenger counts from 1949 to 1960."></div>
                <div class="flashcard" data-question="Why do we normalize data before training an LSTM?" data-answer="LSTMs perform better when inputs are scaled between 0 and 1, avoiding large value disparities that affect gradient updates."></div>
                <div class="flashcard" data-question="What does the reshape function do in LSTMs?" data-answer="It converts the dataset into a 3D array: [samples, time steps, features] required by Keras LSTMs."></div>
                <div class="flashcard" data-question="What is the role of the MinMaxScaler in preprocessing?" data-answer="It scales data to a specified range (typically 0 to 1), improving training stability."></div>
                <div class="flashcard" data-question="What function is used to split data into training and testing sets?" data-answer="The train_test_split function from scikit-learn."></div>
                <div class="flashcard" data-question="What is the purpose of the create_dataset() function?" data-answer="It transforms the dataset into supervised learning format by creating input-output pairs."></div>
                <div class="flashcard" data-question="What activation function is used in LSTM layers by default?" data-answer="The sigmoid activation function is commonly used in LSTMs for gating mechanisms."></div>
                <div class="flashcard" data-question="What loss function is used for regression tasks in LSTMs?" data-answer="Mean Squared Error (MSE) is used to minimize prediction errors."></div>
                <div class="flashcard" data-question="How do we evaluate LSTM model performance?" data-answer="Using Root Mean Squared Error (RMSE) to compare predicted and actual values."></div>
                <div class="flashcard" data-question="What problem is solved in the second part of this lab?" data-answer="SMS spam classification using an LSTM model."></div>
                <div class="flashcard" data-question="How is text data prepared for an LSTM?" data-answer="Text is tokenized using Tokenizer(), converted to sequences, and padded to ensure uniform length."></div>
                <div class="flashcard" data-question="What is embedding in LSTM models?" data-answer="The embedding layer converts words into dense vector representations for better learning."></div>
                <div class="flashcard" data-question="Why is dropout used in LSTM models?" data-answer="Dropout randomly disables neurons during training to reduce overfitting."></div>
                <div class="flashcard" data-question="What activation function is used in the output layer for binary classification?" data-answer="Sigmoid, as it outputs probabilities between 0 and 1."></div>
                <div class="flashcard" data-question="How is model performance measured in spam classification?" data-answer="Using accuracy, loss, and confusion matrices to evaluate predictions."></div>
                <div class="flashcard" data-question="What is early stopping in deep learning?" data-answer="A technique that stops training when validation loss stops improving, preventing overfitting."></div>
                <div class="flashcard" data-question="What is the main takeaway from using LSTMs in this lab?" data-answer="LSTMs can effectively model time-series data and text sequences, improving prediction accuracy."></div>
            </div>
        </section>
        
        <section id="lab9">
            <h2>‚è≥ Lab 9: Long Short-Term Memory (LSTM) Networks</h2>
            <p><strong>Long Short-Term Memory (LSTM)</strong> networks are a type of <strong>Recurrent Neural Network (RNN)</strong> designed to handle <strong>sequential data</strong> while addressing the <strong>vanishing gradient problem</strong>. Unlike standard RNNs, LSTMs use <strong>memory cells and gated mechanisms</strong> to retain long-term dependencies, making them ideal for <strong>time-series forecasting, speech recognition, and natural language processing</strong>.</p>
        
            <h3>üìå 1. Time-Series Forecasting with LSTM</h3>
            <p>We will train an LSTM model to predict <strong>future airline passenger numbers</strong> based on past trends.</p>
        
            <h4>üîπ Loading and Visualizing the Dataset</h4>
            <pre><code>
        from pandas import read_csv
        import matplotlib.pyplot as plt

        # Load dataset (excluding date column)
        dataset = read_csv('international-airline-passengers.csv', usecols=[1], engine='python', skipfooter=3)

        # Plot dataset
        plt.plot(dataset)
        plt.title("International Airline Passengers Over Time")
        plt.xlabel("Months")
        plt.ylabel("Passengers (in thousands)")
        plt.show()
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>This dataset represents <strong>monthly airline passenger counts</strong> over time.</li>
                <li><strong>Time-series forecasting</strong> aims to predict future values by learning patterns from past data.</li>
                <li>Seasonality, trends, and irregularities can significantly affect predictions, making <strong>LSTMs useful for capturing dependencies.</strong></li>
            </ul>
        
            <h4>üîπ Data Normalization</h4>
            <pre><code>
        from sklearn.preprocessing import MinMaxScaler

        # Scale data between 0 and 1
        scaler = MinMaxScaler(feature_range=(0, 1))
        dataset = scaler.fit_transform(dataset)

        print(dataset[:10])  # View first 10 normalized values
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li><strong>Feature scaling</strong> is crucial for neural networks, ensuring numerical values remain in a <strong>consistent range</strong>.</li>
                <li>LSTMs are sensitive to <strong>scale differences</strong>, and normalizing values between <strong>0 and 1</strong> speeds up convergence.</li>
            </ul>
        
            <h4>üîπ Reshaping Data for LSTM</h4>
            <pre><code>
        import numpy as np
        from sklearn.model_selection import train_test_split

        # Split dataset into train and test sets (80/20 split)
        trainX, testX = train_test_split(dataset, test_size=0.2, shuffle=False)

        # Reshape to LSTM input format: [samples, time steps, features]
        trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))
        testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>LSTMs require input in the form <strong>(samples, time steps, features)</strong>.</li>
                <li>Each <strong>sample represents a sequence of past observations</strong>, while the model predicts the next value.</li>
            </ul>
        
            <h4>üîπ Defining and Training an LSTM Model</h4>
            <pre><code>
        from keras.models import Sequential
        from keras.layers import LSTM, Dense

        # Build LSTM model
        model = Sequential()
        model.add(LSTM(50, input_shape=(1, 1)))  # 50 memory cells
        model.add(Dense(1))  # Output layer

        model.compile(loss='mean_squared_error', optimizer='adam')

        # Train model
        model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>The LSTM layer consists of <strong>50 memory cells</strong> that capture <strong>temporal dependencies</strong> in the data.</li>
                <li><code>Dense(1)</code> serves as the output layer, predicting a <strong>single numerical value</strong> (passenger count).</li>
                <li><strong>Mean squared error (MSE)</strong> is used as the loss function for <strong>regression tasks</strong>.</li>
            </ul>
        
            <h3>üìå 2. Spam Classification with LSTM</h3>
            <p>We will now use an <strong>LSTM-based text classifier</strong> to detect spam messages.</p>
        
            <h4>üîπ Text Preprocessing & Tokenization</h4>
            <pre><code>
        from tensorflow.keras.preprocessing.text import Tokenizer
        from tensorflow.keras.preprocessing.sequence import pad_sequences

        # Tokenize text
        tokenizer = Tokenizer(num_words=5000)
        tokenizer.fit_on_texts(X_train)
        sequences = tokenizer.texts_to_sequences(X_train)

        # Pad sequences to ensure uniform input size
        max_len = 150
        X_train_seq = pad_sequences(sequences, maxlen=max_len)
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>Converts text messages into <strong>numerical sequences</strong> using word-to-index mapping.</li>
                <li>Pads sequences to <strong>ensure uniform input size</strong>, as LSTMs require <strong>fixed-length inputs</strong>.</li>
            </ul>
        
            <h4>üîπ Defining and Training the Spam Detection Model</h4>
            <pre><code>
        from keras.layers import Embedding, Dropout, Activation, Input
        from keras.models import Model

        # Define LSTM model
        inputs = Input(shape=[max_len])
        layer = Embedding(5000, 64, input_length=max_len)(inputs)
        layer = LSTM(64)(layer)
        layer = Dense(256, activation='relu')(layer)
        layer = Dropout(0.5)(layer)
        layer = Dense(1, activation='sigmoid')(layer)

        model = Model(inputs, layer)
        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>Uses <strong>word embeddings</strong> to capture the semantic meaning of words.</li>
                <li>The <strong>LSTM layer processes sequential dependencies</strong> in the text.</li>
                <li><strong>Dropout (0.5)</strong> prevents overfitting, and <strong>sigmoid activation</strong> outputs a probability score for spam detection.</li>
            </ul>
        </section>
    </main>
</body>
</html>
