<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AML Lab 9 - Long Short-Term Memory (LSTM) for Sequence Prediction & Classification</title>
    <link rel="stylesheet" href="aml-styles.css">
</head>
<body>
    <header>
        <h1>AML Lab 9 - Long Short-Term Memory (LSTM) for Sequence Prediction & Classification</h1>
    </header>
    
    <main>
        <a href="AML-homepage.html" class="back-button">‚Üê Back to Homepage</a>
        <section id="lab9">
            <h2>‚è≥ Lab 9: Long Short-Term Memory (LSTM) Networks</h2>
            <p><strong>Long Short-Term Memory (LSTM)</strong> networks are a type of <strong>Recurrent Neural Network (RNN)</strong> designed to handle **sequential data** while addressing the **vanishing gradient problem**. Unlike standard RNNs, LSTMs use **memory cells and gated mechanisms** to retain long-term dependencies, making them ideal for **time-series forecasting, speech recognition, and natural language processing**.</p>
        
            <h3>üìå 1. Time-Series Forecasting with LSTM</h3>
            <p>We will train an LSTM model to predict **future airline passenger numbers** based on past trends.</p>
        
            <h4>üîπ Loading and Visualizing the Dataset</h4>
            <pre><code>
        from pandas import read_csv
        import matplotlib.pyplot as plt

        # Load dataset (excluding date column)
        dataset = read_csv('international-airline-passengers.csv', usecols=[1], engine='python', skipfooter=3)

        # Plot dataset
        plt.plot(dataset)
        plt.title("International Airline Passengers Over Time")
        plt.xlabel("Months")
        plt.ylabel("Passengers (in thousands)")
        plt.show()
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>This dataset represents <strong>monthly airline passenger counts</strong> over time.</li>
                <li><strong>Time-series forecasting</strong> aims to predict future values by learning patterns from past data.</li>
                <li>Seasonality, trends, and irregularities can significantly affect predictions, making **LSTMs useful for capturing dependencies.**</li>
            </ul>
        
            <h4>üîπ Data Normalization</h4>
            <pre><code>
        from sklearn.preprocessing import MinMaxScaler

        # Scale data between 0 and 1
        scaler = MinMaxScaler(feature_range=(0, 1))
        dataset = scaler.fit_transform(dataset)

        print(dataset[:10])  # View first 10 normalized values
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li><strong>Feature scaling</strong> is crucial for neural networks, ensuring numerical values remain in a **consistent range**.</li>
                <li>LSTMs are sensitive to **scale differences**, and normalizing values between **0 and 1** speeds up convergence.</li>
            </ul>
        
            <h4>üîπ Reshaping Data for LSTM</h4>
            <pre><code>
        import numpy as np
        from sklearn.model_selection import train_test_split

        # Split dataset into train and test sets (80/20 split)
        trainX, testX = train_test_split(dataset, test_size=0.2, shuffle=False)

        # Reshape to LSTM input format: [samples, time steps, features]
        trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))
        testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>LSTMs require input in the form **(samples, time steps, features)**.</li>
                <li>Each **sample represents a sequence of past observations**, while the model predicts the next value.</li>
            </ul>
        
            <h4>üîπ Defining and Training an LSTM Model</h4>
            <pre><code>
        from keras.models import Sequential
        from keras.layers import LSTM, Dense

        # Build LSTM model
        model = Sequential()
        model.add(LSTM(50, input_shape=(1, 1)))  # 50 memory cells
        model.add(Dense(1))  # Output layer

        model.compile(loss='mean_squared_error', optimizer='adam')

        # Train model
        model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>The LSTM layer consists of **50 memory cells** that capture **temporal dependencies** in the data.</li>
                <li><code>Dense(1)</code> serves as the output layer, predicting a **single numerical value** (passenger count).</li>
                <li><strong>Mean squared error (MSE)</strong> is used as the loss function for **regression tasks**.</li>
            </ul>
        
            <h3>üìå 2. Spam Classification with LSTM</h3>
            <p>We will now use an **LSTM-based text classifier** to detect spam messages.</p>
        
            <h4>üîπ Text Preprocessing & Tokenization</h4>
            <pre><code>
        from tensorflow.keras.preprocessing.text import Tokenizer
        from tensorflow.keras.preprocessing.sequence import pad_sequences

        # Tokenize text
        tokenizer = Tokenizer(num_words=5000)
        tokenizer.fit_on_texts(X_train)
        sequences = tokenizer.texts_to_sequences(X_train)

        # Pad sequences to ensure uniform input size
        max_len = 150
        X_train_seq = pad_sequences(sequences, maxlen=max_len)
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>Converts text messages into **numerical sequences** using word-to-index mapping.</li>
                <li>Pads sequences to **ensure uniform input size**, as LSTMs require **fixed-length inputs**.</li>
            </ul>
        
            <h4>üîπ Defining and Training the Spam Detection Model</h4>
            <pre><code>
        from keras.layers import Embedding, Dropout, Activation, Input
        from keras.models import Model

        # Define LSTM model
        inputs = Input(shape=[max_len])
        layer = Embedding(5000, 64, input_length=max_len)(inputs)
        layer = LSTM(64)(layer)
        layer = Dense(256, activation='relu')(layer)
        layer = Dropout(0.5)(layer)
        layer = Dense(1, activation='sigmoid')(layer)

        model = Model(inputs, layer)
        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>Uses **word embeddings** to capture the semantic meaning of words.</li>
                <li>The **LSTM layer processes sequential dependencies** in the text.</li>
                <li>**Dropout (0.5)** prevents overfitting, and **sigmoid activation** outputs a probability score for spam detection.</li>
            </ul>
        </section>
    </main>
</body>
</html>
