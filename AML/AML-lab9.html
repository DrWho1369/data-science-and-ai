<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AML Lab 9 - Long Short-Term Memory (LSTM) for Sequence Prediction & Classification</title>
    <link rel="stylesheet" href="aml-styles.css">
</head>
<body>
    <header class="hero">
        <h1>AML Lab 9 - Long Short-Term Memory (LSTM) for Sequence Prediction & Classification</h1>
    </header>
    
    <main>
        <a href="AML-homepage.html" class="back-button">‚Üê Back to Homepage</a>
        <section id="lab9">
            <h2>‚è≥ Lab 9: Long Short-Term Memory (LSTM) Networks</h2>
            <p><strong>Long Short-Term Memory (LSTM)</strong> networks are a type of <strong>Recurrent Neural Network (RNN)</strong> designed to handle <strong>sequential data</strong> while addressing the <strong>vanishing gradient problem</strong>. Unlike standard RNNs, LSTMs use <strong>memory cells and gated mechanisms</strong> to retain long-term dependencies, making them ideal for <strong>time-series forecasting, speech recognition, and natural language processing</strong>.</p>
        
            <h3>üìå 1. Time-Series Forecasting with LSTM</h3>
            <p>We will train an LSTM model to predict <strong>future airline passenger numbers</strong> based on past trends.</p>
        
            <h4>üîπ Loading and Visualizing the Dataset</h4>
            <pre><code>
        from pandas import read_csv
        import matplotlib.pyplot as plt

        # Load dataset (excluding date column)
        dataset = read_csv('international-airline-passengers.csv', usecols=[1], engine='python', skipfooter=3)

        # Plot dataset
        plt.plot(dataset)
        plt.title("International Airline Passengers Over Time")
        plt.xlabel("Months")
        plt.ylabel("Passengers (in thousands)")
        plt.show()
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>This dataset represents <strong>monthly airline passenger counts</strong> over time.</li>
                <li><strong>Time-series forecasting</strong> aims to predict future values by learning patterns from past data.</li>
                <li>Seasonality, trends, and irregularities can significantly affect predictions, making <strong>LSTMs useful for capturing dependencies.</strong></li>
            </ul>
        
            <h4>üîπ Data Normalization</h4>
            <pre><code>
        from sklearn.preprocessing import MinMaxScaler

        # Scale data between 0 and 1
        scaler = MinMaxScaler(feature_range=(0, 1))
        dataset = scaler.fit_transform(dataset)

        print(dataset[:10])  # View first 10 normalized values
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li><strong>Feature scaling</strong> is crucial for neural networks, ensuring numerical values remain in a <strong>consistent range</strong>.</li>
                <li>LSTMs are sensitive to <strong>scale differences</strong>, and normalizing values between <strong>0 and 1</strong> speeds up convergence.</li>
            </ul>
        
            <h4>üîπ Reshaping Data for LSTM</h4>
            <pre><code>
        import numpy as np
        from sklearn.model_selection import train_test_split

        # Split dataset into train and test sets (80/20 split)
        trainX, testX = train_test_split(dataset, test_size=0.2, shuffle=False)

        # Reshape to LSTM input format: [samples, time steps, features]
        trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))
        testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>LSTMs require input in the form <strong>(samples, time steps, features)</strong>.</li>
                <li>Each <strong>sample represents a sequence of past observations</strong>, while the model predicts the next value.</li>
            </ul>
        
            <h4>üîπ Defining and Training an LSTM Model</h4>
            <pre><code>
        from keras.models import Sequential
        from keras.layers import LSTM, Dense

        # Build LSTM model
        model = Sequential()
        model.add(LSTM(50, input_shape=(1, 1)))  # 50 memory cells
        model.add(Dense(1))  # Output layer

        model.compile(loss='mean_squared_error', optimizer='adam')

        # Train model
        model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>The LSTM layer consists of <strong>50 memory cells</strong> that capture <strong>temporal dependencies</strong> in the data.</li>
                <li><code>Dense(1)</code> serves as the output layer, predicting a <strong>single numerical value</strong> (passenger count).</li>
                <li><strong>Mean squared error (MSE)</strong> is used as the loss function for <strong>regression tasks</strong>.</li>
            </ul>
        
            <h3>üìå 2. Spam Classification with LSTM</h3>
            <p>We will now use an <strong>LSTM-based text classifier</strong> to detect spam messages.</p>
        
            <h4>üîπ Text Preprocessing & Tokenization</h4>
            <pre><code>
        from tensorflow.keras.preprocessing.text import Tokenizer
        from tensorflow.keras.preprocessing.sequence import pad_sequences

        # Tokenize text
        tokenizer = Tokenizer(num_words=5000)
        tokenizer.fit_on_texts(X_train)
        sequences = tokenizer.texts_to_sequences(X_train)

        # Pad sequences to ensure uniform input size
        max_len = 150
        X_train_seq = pad_sequences(sequences, maxlen=max_len)
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>Converts text messages into <strong>numerical sequences</strong> using word-to-index mapping.</li>
                <li>Pads sequences to <strong>ensure uniform input size</strong>, as LSTMs require <strong>fixed-length inputs</strong>.</li>
            </ul>
        
            <h4>üîπ Defining and Training the Spam Detection Model</h4>
            <pre><code>
        from keras.layers import Embedding, Dropout, Activation, Input
        from keras.models import Model

        # Define LSTM model
        inputs = Input(shape=[max_len])
        layer = Embedding(5000, 64, input_length=max_len)(inputs)
        layer = LSTM(64)(layer)
        layer = Dense(256, activation='relu')(layer)
        layer = Dropout(0.5)(layer)
        layer = Dense(1, activation='sigmoid')(layer)

        model = Model(inputs, layer)
        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>Uses <strong>word embeddings</strong> to capture the semantic meaning of words.</li>
                <li>The <strong>LSTM layer processes sequential dependencies</strong> in the text.</li>
                <li><strong>Dropout (0.5)</strong> prevents overfitting, and <strong>sigmoid activation</strong> outputs a probability score for spam detection.</li>
            </ul>
        </section>
    </main>
</body>
</html>
