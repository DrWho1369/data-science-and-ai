<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Session 9 - Deep Learning 2 (Recurrent Neural Networks - RNNs)</title>
    <link rel="stylesheet" href="aml-styles.css">
</head>
<body>
    <header>
        <h1>Session 9 - Deep Learning 2 (Recurrent Neural Networks - RNNs)</h1>
    </header>
    
    <main>
        <a href="AML-homepage.html" class="back-button">‚Üê Back to Homepage</a>

        <section id="objectives">
            <h2>Key Learning Objectives</h2>
            <ul>
                <li>Understand the fundamentals of Recurrent Neural Networks (RNNs) and their sequential nature.</li>
                <li>Explore the limitations of Vanilla RNNs, including the vanishing gradient problem.</li>
                <li>Learn about Gated RNNs, including LSTMs and GRUs, and how they handle long-term dependencies.</li>
                <li>Understand the concept of Bidirectional RNNs (BRNNs) and their application.</li>
                <li>Explore the Attention mechanism and its role in improving sequence-to-sequence tasks.</li>
                <li>Learn about Explainability in deep learning and how to interpret RNN decisions.</li>
            </ul>
        </section>        
                             
        <section id="flashcards">
            <h2>Flashcards</h2>
            <div class="flashcard-container">
                <div class="flashcard" data-question="What is an RNN?" data-answer="A neural network designed to process sequential data by maintaining a hidden state that captures previous information."></div>
                <div class="flashcard" data-question="Why do Vanilla RNNs struggle with long-term dependencies?" data-answer="Due to the vanishing gradient problem, where gradients shrink as they propagate through long sequences."></div>
                <div class="flashcard" data-question="How do LSTMs solve the vanishing gradient problem?" data-answer="LSTMs use a cell state and gated mechanisms to control how much information is retained or forgotten over time."></div>
                <div class="flashcard" data-question="What are the three gates in an LSTM?" data-answer="Forget gate, input gate, and output gate."></div>
                <div class="flashcard" data-question="What is a GRU?" data-answer="A Gated Recurrent Unit (GRU) is a simplified version of LSTM that combines the forget and input gates into a single update gate."></div>
                <div class="flashcard" data-question="What is a Bidirectional RNN (BRNN)?" data-answer="A model that processes sequence data in both forward and backward directions to capture full context."></div>
                <div class="flashcard" data-question="What is the Attention mechanism?" data-answer="A method that helps RNNs focus on the most relevant parts of an input sequence when making predictions."></div>
                <div class="flashcard" data-question="How does Explainability relate to RNNs?" data-answer="It refers to methods that help interpret RNN decisions, increasing transparency and trust in AI models."></div>
                <div class="flashcard" data-question="What are common applications of RNNs?" data-answer="Speech recognition, sentiment analysis, machine translation, and time-series forecasting."></div>
                <div class="flashcard" data-question="How do RNNs differ from CNNs?" data-answer="RNNs are designed for sequential data, while CNNs are better suited for spatial data like images."></div>
            </div>
        </section>        
                     
        
        <section id="notes">
            <h2>Revision Notes</h2>
            <article>
                <h3>üîπ Recurrent Neural Networks (RNNs)</h3>
                <p>RNNs are neural networks designed for processing sequential data. Unlike traditional feedforward networks, they retain information about previous inputs through their hidden state.</p>
        
                <h3>üîπ Limitations of Vanilla RNNs</h3>
                <ul>
                    <li>Struggle with long-term dependencies due to the <strong>vanishing gradient problem</strong>.</li>
                    <li>Can fail to retain information from earlier parts of a long sequence.</li>
                    <li>Difficulty handling variable-length sequences effectively.</li>
                </ul>
        
                <h3>üîπ Gated RNNs: LSTM & GRU</h3>
                <p>Gated RNNs solve the vanishing gradient issue by introducing mechanisms that regulate information flow.</p>
                <ul>
                    <li><strong>LSTM (Long Short-Term Memory):</strong> Uses a cell state and three gates (forget, input, and output) to maintain long-term dependencies.</li>
                    <li><strong>GRU (Gated Recurrent Unit):</strong> A simplified LSTM with a single update gate, making it computationally more efficient.</li>
                </ul>
        
                <h3>üîπ Bidirectional RNNs (BRNNs)</h3>
                <p>BRNNs process input sequences in both directions, capturing context from past and future words in text-related tasks.</p>
        
                <h3>üîπ Attention Mechanism</h3>
                <p>The Attention mechanism allows RNNs to selectively focus on relevant input parts, improving performance in tasks like translation and speech recognition.</p>
        
                <h3>üîπ Explainability in Deep Learning</h3>
                <p>Understanding why deep learning models make specific predictions is crucial for trust and debugging. Explainability techniques include:</p>
                <ul>
                    <li>Saliency maps</li>
                    <li>Layer-wise relevance propagation</li>
                    <li>SHAP (SHapley Additive exPlanations)</li>
                </ul>
            </article>
        </section>
        
        <section id="key-code-functions">
            <h2>Key Code & Functions</h2>
        
            <h3>1. Implementing a Vanilla RNN</h3>
            <p>A Vanilla RNN processes sequential data by maintaining a hidden state that updates at each time step.</p>
            <pre><code>
        import numpy as np
        
        # Define input, weights, and activation function
        def tanh(x):
            return np.tanh(x)
        
        # Initialize weights and hidden state
        Wxh = np.random.randn(3, 3)  # Input to hidden
        Whh = np.random.randn(3, 3)  # Hidden to hidden
        Why = np.random.randn(3, 3)  # Hidden to output
        bh = np.zeros((1, 3))  # Bias for hidden
        by = np.zeros((1, 3))  # Bias for output
        
        # Sample input
        x_t = np.random.randn(1, 3)
        h_prev = np.zeros((1, 3))
        
        # Compute next hidden state and output
        h_t = tanh(np.dot(x_t, Wxh) + np.dot(h_prev, Whh) + bh)
        y_t = np.dot(h_t, Why) + by
        
        print("Next Hidden State:", h_t)
        print("Output:", y_t)
            </code></pre>
            <p><strong>Explanation:</strong> The RNN updates its hidden state using the previous hidden state and current input. 
            It uses a tanh activation function to regulate values and prevent explosion. The output is computed via matrix multiplication.</p>
        
            <h3>2. Long Short-Term Memory (LSTM)</h3>
            <p>LSTMs introduce gates (input, forget, output) to regulate memory retention and mitigate the vanishing gradient problem.</p>
            <pre><code>
        import tensorflow as tf
        from tensorflow.keras.models import Sequential
        from tensorflow.keras.layers import LSTM, Dense
        
        # Create an LSTM model
        model = Sequential([
            LSTM(50, activation='tanh', return_sequences=True, input_shape=(10, 5)),  # 10 time steps, 5 features
            LSTM(50, activation='tanh'),
            Dense(1, activation='sigmoid')  # Output layer
        ])
        
        # Compile the model
        model.compile(optimizer='adam', loss='binary_crossentropy')
        model.summary()
            </code></pre>
            <p><strong>Explanation:</strong> 
                <ul>
                    <li>The first LSTM layer processes sequential data, maintaining memory over time.</li>
                    <li>The second LSTM layer learns long-term dependencies.</li>
                    <li>A Dense layer with a sigmoid activation is used for binary classification.</li>
                </ul>
            </p>
        
            <h3>3. Gated Recurrent Unit (GRU)</h3>
            <p>GRUs simplify LSTMs by combining the forget and input gates into a single update gate.</p>
            <pre><code>
        model = Sequential([
            GRU(50, activation='tanh', return_sequences=True, input_shape=(10, 5)),  
            GRU(50, activation='tanh'),
            Dense(1, activation='sigmoid') 
        ])
        
        model.compile(optimizer='adam', loss='binary_crossentropy')
        model.summary()
            </code></pre>
            <p><strong>Explanation:</strong> GRUs use update and reset gates, making them computationally more efficient than LSTMs while maintaining memory over long sequences.</p>
        
            <h3>4. Bidirectional RNN</h3>
            <p>Processes sequences in both forward and backward directions, improving context understanding.</p>
            <pre><code>
        from tensorflow.keras.layers import Bidirectional
        
        model = Sequential([
            Bidirectional(LSTM(50, activation='tanh', return_sequences=True), input_shape=(10, 5)),
            Bidirectional(LSTM(50, activation='tanh')),
            Dense(1, activation='sigmoid')
        ])
        
        model.compile(optimizer='adam', loss='binary_crossentropy')
        model.summary()
            </code></pre>
            <p><strong>Explanation:</strong> A Bidirectional RNN processes the input sequence twice: once forward and once backward, improving model accuracy in tasks like NLP.</p>
        
            <h3>5. Attention Mechanism</h3>
            <p>The attention mechanism allows a model to focus on important parts of the input when making predictions.</p>
            <pre><code>
        import tensorflow as tf
        from tensorflow.keras.layers import Attention, Input
        
        query = Input(shape=(10, 64))  # Query input (sequence length 10, feature size 64)
        value = Input(shape=(10, 64))  # Value input
        attention_layer = Attention()([query, value])  # Apply attention mechanism
        
        print("Attention Output Shape:", attention_layer.shape)
            </code></pre>
            <p><strong>Explanation:</strong> The attention mechanism assigns different importance (weights) to parts of an input sequence, improving model focus.</p>
        </section>
        
        
        
    </main>
    
    <footer>
        <p>&copy; 2025 Thomas Baker - MSc Data Science & AI Revision</p>
    </footer>
    
    <script defer src="aml-flashcards.js"></script>
</body>
</html>
