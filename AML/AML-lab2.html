<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AML Lab 2 - Data Preparation</title>
    <link rel="stylesheet" href="aml-styles.css">
</head>
<body>
    <header class="hero">
        <h1>AML Lab 2 - Data Preparation</h1>
    </header>
    
    <main>
        <a href="AML-homepage.html" class="back-button">‚Üê Back to Homepage</a>

        <section id="lab2">
            <h2>üõ†Ô∏è Lab 2: Data Preparation</h2>
            <p>Properly preparing data is a crucial step in Machine Learning. This lab explores <strong>data visualization, scaling, and transformations</strong> to improve model performance. We will use the <strong>Pima Indians Diabetes Dataset</strong> again.</p>
        
            <h3>üìå Univariate Data Visualization</h3>
            <p>Understanding data distribution helps detect <strong>skewness, outliers, and normalization needs</strong>.</p>
        
            <h4>üîπ Histograms</h4>
            <p>Histograms provide a <strong>quick way</strong> to understand data distributions by grouping values into bins.</p>
            <pre><code>
from matplotlib import pyplot
from pandas import read_csv

filename = 'pima-indians-diabetes.data.csv'
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(filename, names=names)

data.hist(figsize=(10,8))
pyplot.show()
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>Histograms reveal if data is <strong>normally distributed, skewed, or contains outliers</strong>.</li>
                <li><strong>Skewed data</strong> may require transformations (e.g., log or power transformation).</li>
                <li><code>figsize=(10,8)</code> ensures clarity when viewing multiple histograms.</li>
            </ul>
        
            <h4>üîπ Density Plots</h4>
            <p>Density plots provide <strong>smoothed histograms</strong>, making it easier to visualize <strong>distribution trends</strong>.</p>
            <pre><code>
data.plot(kind='density', subplots=True, layout=(3,3), sharex=False, figsize=(12,10))
pyplot.show()
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>Reveals <strong>skewed distributions</strong>, helping determine whether transformations are needed.</li>
                <li>Useful for detecting <strong>overlapping features that may not contribute well</strong> to the model.</li>
            </ul>
        
            <h3>üìå Multivariate Data Visualization</h3>
            <p>Shows relationships between variables to detect <strong>redundant or highly correlated features</strong>.</p>
        
            <h4>üîπ Correlation Matrix Plot</h4>
            <p>Helps identify <strong>highly correlated features</strong> that could cause multicollinearity issues.</p>
            <pre><code>
from matplotlib import pyplot
import numpy as np

correlations = data.corr()
fig = pyplot.figure(figsize=(8,8))
ax = fig.add_subplot(111)
cax = ax.matshow(correlations, vmin=-1, vmax=1)
fig.colorbar(cax)

ticks = np.arange(0,9,1)
ax.set_xticks(ticks)
ax.set_yticks(ticks)
ax.set_xticklabels(names)
ax.set_yticklabels(names)

pyplot.show()
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>Correlation values close to <strong>1</strong> indicate strong positive correlation.</li>
                <li>Features that are <strong>highly correlated</strong> should be considered for removal to reduce redundancy.</li>
            </ul>
        
            <h3>üìå Data Scaling & Normalization</h3>
            <p>Scaling ensures that features contribute <strong>equally</strong> to model predictions.</p>
        
            <h4>üîπ Min-Max Scaling</h4>
            <p>Rescales values to be <strong>between 0 and 1</strong>, useful for <strong>distance-based models like KNN</strong>.</p>
            <pre><code>
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler(feature_range=(0, 1))
rescaledX = scaler.fit_transform(data.iloc[:, :-1])
print(rescaledX[:5])  # Print first 5 rows
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>Ensures all features are in a <strong>standard range</strong>, preventing dominance of larger-scale features.</li>
                <li>Improves <strong>convergence in gradient-based algorithms</strong> like neural networks.</li>
            </ul>
        
            <h4>üîπ Standardization</h4>
            <p>Transforms data to have <strong>mean = 0</strong> and <strong>standard deviation = 1</strong>.</p>
            <pre><code>
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
standardizedX = scaler.fit_transform(data.iloc[:, :-1])
print(standardizedX[:5])
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>Standardization is critical for <strong>Logistic Regression and SVMs</strong>, which are sensitive to feature scale.</li>
            </ul>
        
            <h4>üîπ Normalization</h4>
            <p>Scales each <strong>row</strong> (not column) to <strong>unit norm</strong>, useful for sparse data.</p>
            <pre><code>
from sklearn.preprocessing import Normalizer

scaler = Normalizer()
normalizedX = scaler.fit_transform(data.iloc[:, :-1])
print(normalizedX[:5])
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>Ensures that each <strong>sample contributes equally</strong> to model training.</li>
            </ul>
        
            <h3>üìå Comparing Decision Tree Performance on Raw vs. Normalized Data</h3>
            <pre><code>
from sklearn.model_selection import cross_val_score, KFold
from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier()
kfold = KFold(n_splits=10, random_state=7, shuffle=True)

raw_scores = cross_val_score(model, data.iloc[:, :-1], data.iloc[:, -1], cv=kfold)
normalized_scores = cross_val_score(model, normalizedX, data.iloc[:, -1], cv=kfold)

print(f"Raw Data Accuracy: {raw_scores.mean():.3f}")
print(f"Normalized Data Accuracy: {normalized_scores.mean():.3f}")
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>Compares the impact of <strong>normalization on decision tree performance</strong>.</li>
                <li>If normalization <strong>improves accuracy</strong>, then it‚Äôs beneficial for this dataset.</li>
            </ul>
        </section>
    </main>
    
    <footer>
        <p>&copy; 2025 Thomas Baker - MSc Data Science & AI Revision</p>
    </footer>
    
    <script defer src="aml-flashcards.js"></script>
</body>
</html>
