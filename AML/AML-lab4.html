<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AML Lab 4 - Comparing Machine Learning Algorithms</title>
    <link rel="stylesheet" href="aml-styles.css">
</head>
<body>
    <header class="hero">
        <h1>AML Lab 4 - Comparing Machine Learning Algorithms</h1>
    </header>
    
    <main>
        <a href="AML-homepage.html" class="back-button">‚Üê Back to Homepage</a>
        <section id="flashcards">
            <h2>Flashcards</h2>
            <div class="flashcard-container">
                <div class="flashcard" data-question="What is classification accuracy?" data-answer="It is the number of correct predictions divided by the total number of predictions."></div>
                <div class="flashcard" data-question="Why is classification accuracy sometimes misleading?" data-answer="It does not account for class imbalances or the cost of different types of errors."></div>
                <div class="flashcard" data-question="What is Logarithmic Loss (LogLoss)?" data-answer="A metric that evaluates the confidence of a probabilistic classification model. Lower values are better."></div>
                <div class="flashcard" data-question="What does a high LogLoss indicate?" data-answer="That the predicted probabilities are far from the actual labels."></div>
                <div class="flashcard" data-question="What is the Area Under the ROC Curve (AUC)?" data-answer="A metric that measures a model's ability to distinguish between positive and negative classes."></div>
                <div class="flashcard" data-question="What does an AUC value of 1.0 mean?" data-answer="The model perfectly classifies positive and negative instances."></div>
                <div class="flashcard" data-question="What is a confusion matrix?" data-answer="A table showing the number of correct and incorrect predictions for each class."></div>
                <div class="flashcard" data-question="What are Type I and Type II errors?" data-answer="Type I is a false positive, Type II is a false negative."></div>
                <div class="flashcard" data-question="How do you interpret precision in a classification report?" data-answer="Precision is the proportion of true positive predictions among all positive predictions."></div>
                <div class="flashcard" data-question="How do you interpret recall in a classification report?" data-answer="Recall measures how many actual positives were correctly identified by the model."></div>
                <div class="flashcard" data-question="What is the F1-score?" data-answer="A harmonic mean of precision and recall, balancing false positives and false negatives."></div>
                <div class="flashcard" data-question="What is cross-validation?" data-answer="A resampling technique used to evaluate machine learning models by splitting the data into multiple training and testing sets."></div>
                <div class="flashcard" data-question="Why do we use k-fold cross-validation?" data-answer="To reduce variance in model evaluation by training and testing on multiple splits of the dataset."></div>
                <div class="flashcard" data-question="What is the main purpose of evaluating multiple ML models?" data-answer="To identify the model that performs best on a given dataset."></div>
                <div class="flashcard" data-question="Which models were compared in Lab 4?" data-answer="Logistic Regression, K-Nearest Neighbors, Decision Trees, SVM, Na√Øve Bayes, and LDA."></div>
                <div class="flashcard" data-question="What does the ROC curve show?" data-answer="The trade-off between sensitivity (recall) and specificity for different classification thresholds."></div>
                <div class="flashcard" data-question="What does a classification report summarize?" data-answer="Precision, recall, F1-score, and support for each class."></div>
                <div class="flashcard" data-question="What is a good AUC score?" data-answer="Typically, AUC > 0.8 is considered good, while > 0.9 is excellent."></div>
                <div class="flashcard" data-question="How do we compare machine learning models visually?" data-answer="Using boxplots to compare accuracy distributions across cross-validation folds."></div>
                <div class="flashcard" data-question="What is the purpose of pruning in decision trees?" data-answer="To prevent overfitting by limiting tree depth and reducing complexity."></div>
            </div>
        </section>
        
        <section id="lab4">
            <h2>‚öñÔ∏è Lab 4: Comparing Machine Learning Algorithms</h2>
            <p>In machine learning, choosing the right model is essential for achieving high performance. This lab compares multiple <strong>classification algorithms</strong> using various evaluation metrics on the <strong>Pima Indians Diabetes Dataset</strong>.</p>
        
            <h3>üìå 1. Evaluating Classification Accuracy</h3>
            <p><strong>Accuracy</strong> is the most basic metric, but it can be misleading for <strong>imbalanced datasets</strong>.</p>
            <pre><code>
from sklearn.model_selection import KFold, cross_val_score
from sklearn.linear_model import LogisticRegression

# Define 10-fold cross-validation
kfold = KFold(n_splits=10, random_state=7, shuffle=True)
model = LogisticRegression(solver='liblinear')

# Compute accuracy
results = cross_val_score(model, X, Y, cv=kfold, scoring='accuracy')
print(f"Accuracy: {results.mean():.3f} ({results.std():.3f})")
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li><strong>Cross-validation (CV):</strong> Splits data into 10 subsets, ensuring each part is used for training and validation.</li>
                <li><strong>Accuracy:</strong> Measures the percentage of correct predictions but doesn't distinguish false positives from false negatives.</li>
                <li><strong>Standard deviation:</strong> Indicates variability in accuracy scores across different validation sets.</li>
            </ul>
        
            <h3>üìå 2. Logarithmic Loss (Log Loss)</h3>
            <p><strong>Log Loss</strong> penalizes incorrect predictions based on how confident the model was.</p>
            <pre><code>
from sklearn.metrics import log_loss

# Compute Log Loss using cross-validation
results = cross_val_score(model, X, Y, cv=kfold, scoring='neg_log_loss')
print(f"Log Loss: {-results.mean():.3f} ({results.std():.3f})")
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li><strong>Lower Log Loss:</strong> Indicates better probability calibration.</li>
                <li>Unlike accuracy, Log Loss <strong>accounts for prediction confidence</strong>.</li>
            </ul>
        
            <h3>üìå 3. Area Under ROC Curve (AUC-ROC)</h3>
            <p>The <strong>ROC curve</strong> plots <strong>true positive rate (TPR)</strong> vs. <strong>false positive rate (FPR)</strong>. The <strong>AUC score</strong> summarizes model performance across all classification thresholds.</p>
            <pre><code>
results = cross_val_score(model, X, Y, cv=kfold, scoring='roc_auc')
print(f"AUC: {results.mean():.3f} ({results.std():.3f})")
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li><strong>AUC closer to 1:</strong> Indicates better ability to rank positive instances ahead of negatives.</li>
                <li>Works well on <strong>imbalanced datasets</strong> where accuracy is unreliable.</li>
            </ul>
        
            <h3>üìå 4. Confusion Matrix & Classification Report</h3>
            <p>A confusion matrix provides detailed breakdowns of model performance.</p>
            <pre><code>
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import train_test_split

# Train-test split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=7)
model.fit(X_train, Y_train)

# Predictions and evaluation
predicted = model.predict(X_test)
matrix = confusion_matrix(Y_test, predicted)
report = classification_report(Y_test, predicted)
print(matrix)
print(report)
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li><strong>Confusion matrix:</strong> Breaks down true positives, false positives, true negatives, and false negatives.</li>
                <li><strong>Precision & Recall:</strong> Precision measures how many predicted positives are actually positive, while recall measures how many actual positives were detected.</li>
                <li><strong>F1-score:</strong> Balances precision and recall to evaluate model performance.</li>
            </ul>
        
            <h3>üìå 5. Comparing Multiple Models</h3>
            <p>We compare six models using cross-validation.</p>
            <pre><code>
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

# Define models
models = []
models.append(('LR', LogisticRegression(solver='liblinear')))
models.append(('LDA', LinearDiscriminantAnalysis()))
models.append(('KNN', KNeighborsClassifier()))
models.append(('CART', DecisionTreeClassifier()))
models.append(('NB', GaussianNB()))
models.append(('SVM', SVC()))
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>We compare <strong>Logistic Regression, LDA, KNN, Decision Tree, Naive Bayes, and SVM</strong>.</li>
                <li>Different models excel in different scenarios:</li>
                <ul>
                    <li><strong>Decision Trees:</strong> Handle nonlinear data but prone to overfitting.</li>
                    <li><strong>Naive Bayes:</strong> Assumes independence between features, making it fast but limited.</li>
                    <li><strong>KNN:</strong> Works well on small datasets but struggles with large feature spaces.</li>
                </ul>
            </ul>
        
            <h3>üìå Summary</h3>
            <ul>
                <li>Used <strong>cross-validation</strong> to compare models fairly.</li>
                <li>Evaluated performance using <strong>Accuracy, Log Loss, AUC-ROC, and Confusion Matrices.</strong></li>
                <li>Different models suit different datasets‚Äî<strong>no single algorithm works best in all cases</strong>.</li>
            </ul>
        </section>
    </main>
    
    <footer>
        <p>&copy; 2025 Thomas Baker - MSc Data Science & AI Revision</p>
    </footer>
    
    <script defer src="aml-flashcards.js"></script>
</body>
</html>
