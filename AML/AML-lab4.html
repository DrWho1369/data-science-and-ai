<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AML Lab 4 - Comparing Machine Learning Algorithms</title>
    <link rel="stylesheet" href="aml-styles.css">
</head>
<body>
    <header>
        <h1>AML Lab 4 - Comparing Machine Learning Algorithms</h1>
    </header>
    
    <main>
        <a href="AML-homepage.html" class="back-button">‚Üê Back to Homepage</a>

        <section id="lab4">
            <h2>‚öñÔ∏è Lab 4: Comparing Machine Learning Algorithms</h2>
            <p>In machine learning, choosing the right model is essential for achieving high performance. This lab compares multiple <strong>classification algorithms</strong> using various evaluation metrics on the <strong>Pima Indians Diabetes Dataset</strong>.</p>
        
            <h3>üìå 1. Evaluating Classification Accuracy</h3>
            <p><strong>Accuracy</strong> is the most basic metric, but it can be misleading for <strong>imbalanced datasets</strong>.</p>
            <pre><code>
from sklearn.model_selection import KFold, cross_val_score
from sklearn.linear_model import LogisticRegression

# Define 10-fold cross-validation
kfold = KFold(n_splits=10, random_state=7, shuffle=True)
model = LogisticRegression(solver='liblinear')

# Compute accuracy
results = cross_val_score(model, X, Y, cv=kfold, scoring='accuracy')
print(f"Accuracy: {results.mean():.3f} ({results.std():.3f})")
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li><strong>Cross-validation (CV):</strong> Splits data into 10 subsets, ensuring each part is used for training and validation.</li>
                <li><strong>Accuracy:</strong> Measures the percentage of correct predictions but doesn't distinguish false positives from false negatives.</li>
                <li><strong>Standard deviation:</strong> Indicates variability in accuracy scores across different validation sets.</li>
            </ul>
        
            <h3>üìå 2. Logarithmic Loss (Log Loss)</h3>
            <p><strong>Log Loss</strong> penalizes incorrect predictions based on how confident the model was.</p>
            <pre><code>
from sklearn.metrics import log_loss

# Compute Log Loss using cross-validation
results = cross_val_score(model, X, Y, cv=kfold, scoring='neg_log_loss')
print(f"Log Loss: {-results.mean():.3f} ({results.std():.3f})")
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li><strong>Lower Log Loss:</strong> Indicates better probability calibration.</li>
                <li>Unlike accuracy, Log Loss <strong>accounts for prediction confidence</strong>.</li>
            </ul>
        
            <h3>üìå 3. Area Under ROC Curve (AUC-ROC)</h3>
            <p>The <strong>ROC curve</strong> plots <strong>true positive rate (TPR)</strong> vs. <strong>false positive rate (FPR)</strong>. The <strong>AUC score</strong> summarizes model performance across all classification thresholds.</p>
            <pre><code>
results = cross_val_score(model, X, Y, cv=kfold, scoring='roc_auc')
print(f"AUC: {results.mean():.3f} ({results.std():.3f})")
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li><strong>AUC closer to 1:</strong> Indicates better ability to rank positive instances ahead of negatives.</li>
                <li>Works well on <strong>imbalanced datasets</strong> where accuracy is unreliable.</li>
            </ul>
        
            <h3>üìå 4. Confusion Matrix & Classification Report</h3>
            <p>A confusion matrix provides detailed breakdowns of model performance.</p>
            <pre><code>
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import train_test_split

# Train-test split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=7)
model.fit(X_train, Y_train)

# Predictions and evaluation
predicted = model.predict(X_test)
matrix = confusion_matrix(Y_test, predicted)
report = classification_report(Y_test, predicted)
print(matrix)
print(report)
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li><strong>Confusion matrix:</strong> Breaks down true positives, false positives, true negatives, and false negatives.</li>
                <li><strong>Precision & Recall:</strong> Precision measures how many predicted positives are actually positive, while recall measures how many actual positives were detected.</li>
                <li><strong>F1-score:</strong> Balances precision and recall to evaluate model performance.</li>
            </ul>
        
            <h3>üìå 5. Comparing Multiple Models</h3>
            <p>We compare six models using cross-validation.</p>
            <pre><code>
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

# Define models
models = []
models.append(('LR', LogisticRegression(solver='liblinear')))
models.append(('LDA', LinearDiscriminantAnalysis()))
models.append(('KNN', KNeighborsClassifier()))
models.append(('CART', DecisionTreeClassifier()))
models.append(('NB', GaussianNB()))
models.append(('SVM', SVC()))
            </code></pre>
            <p><strong>Deep Explanation:</strong></p>
            <ul>
                <li>We compare <strong>Logistic Regression, LDA, KNN, Decision Tree, Naive Bayes, and SVM</strong>.</li>
                <li>Different models excel in different scenarios:</li>
                <ul>
                    <li><strong>Decision Trees:</strong> Handle nonlinear data but prone to overfitting.</li>
                    <li><strong>Naive Bayes:</strong> Assumes independence between features, making it fast but limited.</li>
                    <li><strong>KNN:</strong> Works well on small datasets but struggles with large feature spaces.</li>
                </ul>
            </ul>
        
            <h3>üìå Summary</h3>
            <ul>
                <li>Used <strong>cross-validation</strong> to compare models fairly.</li>
                <li>Evaluated performance using <strong>Accuracy, Log Loss, AUC-ROC, and Confusion Matrices.</strong></li>
                <li>Different models suit different datasets‚Äî**no single algorithm works best in all cases**.</li>
            </ul>
        </section>
    </main>
    
    <footer>
        <p>&copy; 2025 Thomas Baker - MSc Data Science & AI Revision</p>
    </footer>
    
    <script defer src="aml-flashcards.js"></script>
</body>
</html>
